<!DOCTYPE html>
<html lang="en-GB">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="index, follow">

    <!-- DNS Prefetch and Preconnect for Performance -->
    <link rel="dns-prefetch" href="//fonts.googleapis.com">
    <link rel="dns-prefetch" href="//cdnjs.cloudflare.com">
    <link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>

    <!-- SEO Meta Tags -->
    <title>Memory-Driven Agentic AI for HAR | ICSR 2025</title>
    <meta name="description" content="Agentic AI for human activity recognition with memory-driven continual learning and context-aware reasoning for human-robot interaction. ICSR 2025.">
    <meta name="keywords" content="human activity recognition, HAR, continual learning, memory-driven AI, agentic AI, human-robot interaction, HRI, reinforcement learning, explainable AI, context awareness, ICSR 2025, Khashayar Ghamati, adaptive robotics, vision language model, supervised learning, machine learning, AI conference, neuromorphic computing, cognitive architectures">

    <!-- Open Graph -->
    <meta property="og:title" content="Memory-Driven Agentic AI for HAR | ICSR 2025">
    <meta property="og:description" content="Agentic AI architecture for robust, explainable human activity recognition in HRI — fusing memory, continual learning, and context-aware reasoning. ICSR 2025.">
    <meta property="og:image" content="https://ghamati.com/har-agent/main-arch.PNG">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://ghamati.com/har-agent/">
    <meta property="og:site_name" content="Khashayar Ghamati - AI Research">
    <meta property="og:locale" content="en_GB">

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Memory-Driven Agentic AI for HAR | ICSR 2025">
    <meta name="twitter:description" content="ICSR 2025: Continual learning & memory-driven human activity recognition in HRI.">
    <meta name="twitter:image" content="https://ghamati.com/har-agent/main-arch.PNG">

    <link rel="canonical" href="https://ghamati.com/har-agent/">

    <!-- Google Fonts -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Playfair+Display:wght@400;600;700&display=swap">
    <!-- Material Design 3 Shared CSS -->
    <link rel="stylesheet" href="/css/material-design.css">
    <!-- External CSS for Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer">

    <script type="application/ld+json">
        {
            "@context": "https://schema.org",
            "@type": "ScholarlyArticle",
            "headline": "Towards Memory-Driven Agentic AI for Human Activity Recognition",
            "author": [{
                "@type": "Person",
                "name": "Khashayar Ghamati",
                "affiliation": "University of Hertfordshire"
            }],
            "datePublished": "2025-09-05",
            "publisher": {
                "@type": "Organization",
                "name": "ICSR Conference"
            },
            "about": [
                "human activity recognition",
                "HAR",
                "continual learning",
                "memory-driven AI",
                "agentic AI",
                "human-robot interaction",
                "reinforcement learning"
            ],
            "description": "This paper introduces an agentic AI framework for robust human activity recognition in HRI, leveraging memory, context-aware reasoning, and continual learning. Accepted for presentation at ICSR 2025."
        }
    </script>

    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-HV1K905FF2"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-HV1K905FF2');
    </script>

    <style>
        .md-highlight-box .fas { margin-right: 0.5rem; }
        .md-figure--multi img { margin-bottom: 1rem; }
        .md-figure--multi img:last-of-type { margin-bottom: 0; }
    </style>
</head>
<body>

<a href="#main-content" class="md-skip-link">Skip to main content</a>

<!-- Top App Bar -->
<nav class="md-top-app-bar" id="top-bar">
    <div class="md-top-app-bar__nav">
        <a href="/"><i class="fas fa-home"></i> Home</a>
    </div>
</nav>

<!-- Hero Section -->
<header class="md-hero">
    <h1 class="md-hero__title">Towards Memory-Driven Agentic AI for Human Activity Recognition</h1>
</header>

<main id="main-content">
    <div class="md-container">
        <section class="md-section md-card-elevated md-mt-32">
            <h2 class="md-headline-large">About the Paper</h2>
            <p class="md-body-large">
                <strong>Towards Memory-Driven Agentic AI for Human Activity Recognition</strong> introduces a next-generation agentic AI architecture, designed to revolutionise <b>human activity recognition (HAR)</b> in the context of <b>human-robot interaction (HRI)</b>. By unifying advanced sensing, context-aware reasoning, and both short- and long-term memory modules, our approach enables robots to move beyond static pattern matching — helping them dynamically interpret and adapt to human behaviours, even in unpredictable or unseen situations.
            </p>
            <p class="md-body-large">
                <b>Agentic AI</b> marks a shift from traditional, rule-based AI by allowing systems to learn continuously from experience. Our work positions agentic AI as a key enabler for flexible and socially-aware robotics, especially for assistive, collaborative, and adaptive real-world applications.
            </p>

            <div class="md-highlight-box">
                <i class="fas fa-lightbulb"></i> <b>Did you know?</b> Most current AI systems struggle to distinguish between actions that "look" similar (e.g., standing up vs. lifting an object), especially when context or environmental cues are missing.
            </div>
        </section>

        <div class="md-figure md-mt-32">
            <img src="main-arch.PNG" alt="Memory-driven agentic AI architecture for continual human activity recognition, context-aware reasoning, and human-robot interaction, ICSR 2025" loading="eager" width="600" height="450">
            <p class="md-figure__caption"><b>Figure 1:</b> Overview of the proposed Agentic AI architecture. Data flows from multi-modal sensors into parallel processing streams: supervised learning (for skeleton-based HAR), environmental context extraction, and memory modules. Short-term memory (STM) captures recent activity transitions, while long-term memory (LTM) encodes experiential knowledge across sessions — enabling true adaptation.</p>
        </div>

        <section class="md-section md-card-elevated">
            <h2 class="md-headline-large">Motivation</h2>
            <p class="md-body-large">
                Human behaviours are diverse, ambiguous, and context-dependent. Traditional HAR approaches are usually "brittle" — relying on predefined models, and easily fooled by changes in viewpoint, overlapping actions, or subtle shifts in context. For example, a person reaching for a cup and simply raising their hand may look similar to a robot, but the intended action is completely different!
            </p>
            <p class="md-body-large">
                Our work addresses this by building in <b>short-term memory</b> (for activity transitions) and <b>long-term memory</b> (for accumulated experience), inspired by human cognitive processes. This allows the agent to reason over time and make more robust inferences.
            </p>

            <h3 class="md-headline-medium">Memory Architecture Components</h3>
            <ul>
                <li class="md-body-large"><b>Short-term memory (STM):</b> Aggregates information across frames to capture temporal changes and activity transitions.</li>
                <li class="md-body-large"><b>Long-term memory (LTM):</b> Stores key experiences and contextual knowledge, allowing the system to generalise across different environments and users.</li>
            </ul>

            <div class="md-highlight-box">
                <i class="fas fa-info-circle"></i> <b>Why is this important?</b> In real-world settings — such as healthcare, elderly care, or collaborative robotics — robots must understand not just what humans are doing, but <i>why</i>, and respond in contextually appropriate ways. Our architecture is a step towards that goal.
            </div>
        </section>

        <section class="md-section md-card-elevated">
            <h2 class="md-headline-large">How the Architecture Works</h2>
            <ul>
                <li class="md-body-large"><strong>Sensing Layer:</strong> Acquires multimodal data (e.g., video, skeletons) and preprocesses it for downstream analysis. We benchmarked using challenging real-world datasets: <b>RHM</b> (for training, with robot-centric views) and <b>Toyota Smart Home</b> (for out-of-distribution testing).</li>
                <li class="md-body-large"><strong>Deliberative Reasoning:</strong> The heart of the system. It brings together:
                    <ul>
                        <li>Supervised detection (<b>M-LeNet</b> for skeletons) — robust to known activities.</li>
                        <li><b>Perceptor + LLaVA</b>: Extracts high-level semantic descriptions from image frames, incorporating contextual cues using large vision-language models.</li>
                        <li>Memory modules: STM tracks transitions; LTM accumulates "life-long" knowledge of activities and their contexts.</li>
                    </ul>
                </li>
                <li class="md-body-large"><strong>Context Alignment (CA):</strong> This probabilistic module integrates outputs from supervised models and context extractors. By using conditional probabilities and the <b>Jaccard Index</b>, CA refines ambiguous predictions and enables robust recognition, especially for overlapping or ambiguous activities.</li>
            </ul>

            <div class="md-highlight-box">
                <i class="fas fa-cogs"></i> <b>Technical innovation:</b> Unlike typical HAR, our agent continually aligns its interpretation of activities using both its experience (LTM) and recent context (STM). This hybrid approach substantially improves generalisation and robustness.
            </div>
        </section>

        <section class="md-section md-card-elevated" id="exp">
            <h2 class="md-headline-large">Video Explanation</h2>
            <div class="md-card-outlined md-mt-24" style="text-align:center; padding:24px;">
                <iframe src="https://www.youtube.com/embed/pB8x4oMSqqY" title="Towards Memory-Driven Agentic AI for Human Activity Recognition" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen loading="lazy" style="width:100%; max-width:560px; height:315px; border:none; border-radius:var(--md-shape-md);"></iframe>
            </div>
        </section>

        <div class="md-figure">
            <img src="a2.png" alt="Context alignment using short-term and long-term memory in agentic AI for human activity recognition, ICSR 2025" loading="lazy" width="550" height="400">
            <p class="md-figure__caption"><b>Figure 2:</b> The context alignment module in action. The agent updates its STM as it observes a sequence of frames, allowing it to refine an ambiguous prediction ("standing up") into a more precise one ("lifting an object") by integrating context and memory.</p>
        </div>

        <section class="md-section md-card-elevated">
            <h2 class="md-headline-large">Evaluation on Real-World HRI Datasets</h2>
            <p class="md-body-large">
                We rigorously evaluated the proposed architecture on two major datasets:
            </p>
            <ul>
                <li class="md-body-large"><b>RHM:</b> Multi-view robot-centric HAR dataset (used for training and initial validation).</li>
                <li class="md-body-large"><b>Toyota Smart Home:</b> Large-scale, real-world dataset with challenging unseen activities (used for testing generalisation).</li>
            </ul>
            <ul>
                <li class="md-body-large"><b>On unseen data:</b> Combining context and supervised model predictions achieved <b>60% accuracy</b> — a dramatic improvement over context-only (40%) or supervised-only (35%) approaches.</li>
                <li class="md-body-large"><b>Generalisation:</b> The agent correctly adapted to unfamiliar activities and new environments, demonstrating robustness beyond the training set.</li>
                <li class="md-body-large"><b>Activity transition awareness:</b> STM enabled the agent to detect and track activity changes over time, a key capability for real-world HRI.</li>
            </ul>

            <div class="md-highlight-box">
                <i class="fas fa-chart-line"></i> <b>Takeaway:</b> The fusion of context, memory, and supervision not only boosts accuracy but also makes the agent <b>explainable</b> — we can trace which contextual cues or past experiences led to a particular decision.
            </div>
        </section>

        <div class="md-figure">
            <img src="a1.png" alt="Generalisation to unseen human activities by memory-driven agentic AI, HAR, and HRI" loading="lazy" width="550" height="400">
            <p class="md-figure__caption"><b>Figure 3:</b> Generalisation example. Here, the agent revises its initial (incorrect) prediction from "cleaning" to "carrying object" as new context and STM updates become available — demonstrating cross-domain adaptability.</p>
        </div>

        <section class="md-section md-card-elevated">
            <h2 class="md-headline-large">Why This Matters</h2>
            <p class="md-body-large">
                Building <b>trustworthy, context-aware, and adaptive AI agents</b> is essential for the next generation of human-robot collaboration. Our memory-driven agentic AI framework advances the state of the art in several ways:
            </p>
            <ul>
                <li class="md-body-large">Improves safety and reliability by reducing misinterpretation of human actions.</li>
                <li class="md-body-large">Enables robots to collaborate with humans more naturally, anticipating needs and responding proactively.</li>
                <li class="md-body-large">Provides a foundation for life-long learning and adaptation in robots deployed in dynamic, open-ended environments.</li>
            </ul>

            <div class="md-highlight-box">
                <i class="fas fa-robot"></i> <b>Impact:</b> From healthcare and assistive living to industrial and service robotics, these advancements bring us closer to robots that understand, learn from, and genuinely help people — adapting to their habits and preferences over time.
            </div>
        </section>

        <div class="md-figure md-figure--multi">
            <img src="b1.png" alt="Successful STM-based inference, human activity recognition with context memory, agentic AI" loading="lazy" width="400" height="300">
            <p class="md-figure__caption"><b>a)</b> Correct prediction: The image generated by ChatGPT from STM descriptions closely matches the actual video sequence, showing successful context capture.</p>
            <img src="b2.png" alt="Failed STM-based inference, challenges in human activity recognition with agentic AI and memory context" loading="lazy" width="400" height="300">
            <p class="md-figure__caption"><b>b)</b> Incorrect prediction: Vague STM content leads to a poor match and low confidence in activity recognition.</p>
            <p class="md-figure__caption"><b>Figure 4:</b> STM-based inference comparison: these examples show the importance of rich context for accurate and trustworthy agentic AI.</p>
        </div>

        <section class="md-section md-card-elevated">
            <h2 class="md-headline-large">Open Science & Collaboration</h2>
            <p class="md-body-large">
                We believe in open research. If you are interested in collaborating, adapting our methods, or accessing the code or datasets, please feel free to contact us. Our goal is to accelerate progress towards agentic, explainable AI for real-world robotics.
            </p>

            <div class="md-social-links" style="justify-content:center;">
                <a href="https://scholar.google.com/citations?user=zRG8t_oAAAAJ&hl=en" target="_blank" rel="noopener" title="Google Scholar">
                    <i class="fas fa-graduation-cap"></i>
                </a>
                <a href="https://orcid.org/0009-0002-6416-3127" target="_blank" rel="noopener" title="ORCID">
                    <i class="fab fa-orcid"></i>
                </a>
            </div>
        </section>

        <section class="md-section md-card-elevated">
            <h2 class="md-headline-large">Full Paper & Code</h2>
            <div style="text-align:center; padding:24px 0;">
                <a href="#" class="md-btn-filled" onclick="showPaperDialog(event)">
                    <i class="fas fa-download"></i> Download Full Paper
                </a>
                <a href="#" class="md-btn-outlined" onclick="showCodeDialog(event)">
                    <i class="fas fa-code"></i> Access Code
                </a>
            </div>
            <p class="md-body-medium" style="text-align:center; color:var(--md-on-surface-variant);">
                <i class="fas fa-info-circle"></i> The paper will be available in the proceedings of ICSR 2025 after the conference takes place in September 2025.
            </p>
        </section>
    </div>
</main>

<script>
    function showCodeDialog(event) {
        event.preventDefault();
        alert("To access the source code, please email us at: k.ghamati@herts.ac.uk");
    }

    function showPaperDialog(event) {
        event.preventDefault();
        alert("The paper will be available in the proceedings of ICSR 2025 after the conference takes place in September 2025.");
    }

    // Top app bar scroll elevation
    window.addEventListener('scroll', function() {
        document.getElementById('top-bar').classList.toggle('md-scrolled', window.scrollY > 0);
    });
</script>

<footer style="background-color:var(--md-inverse-surface); color:var(--md-inverse-on-surface); text-align:center; padding:48px 24px;">
    <div class="md-container">
        <p class="md-body-large">&copy; 2025 Khashayar Ghamati. All rights reserved.</p>
        <p class="md-body-medium" style="opacity:0.8; margin-top:8px;">Advancing agentic AI for human-centred robotics</p>
    </div>
</footer>

</body>
</html>
