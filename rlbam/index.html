<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <!-- DNS Prefetch and Preconnect for Performance -->
  <link rel="dns-prefetch" href="//fonts.googleapis.com">
  <link rel="dns-prefetch" href="//cdnjs.cloudflare.com">
  <link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  
  <meta name="description" content="ARI humanoid robot learns human-like gaze behaviour with reinforcement learning: robust attention modelling, domain randomisation, and zero-shot sim-to-real transfer for social HRI. Published at Humanoids 2024 by Khashayar Ghamati et al.">
  <meta name="keywords" content="humanoid robot, ARI, reinforcement learning, gaze behaviour, attention modelling, sim-to-real transfer, domain randomisation, zero-shot transfer, PPO, human-robot interaction, HRI, social robotics, deep RL, generalisation, real-world robotics, University of Hertfordshire, Khashayar Ghamati, Humanoids 2024, social gaze, robot attention, saliency, ROS, Kinect, RL transfer learning, PPO tuning">
  <meta name="author" content="Khashayar Ghamati, Abolfazl Zaraki, Farshid Amirabdollahian">

  <meta property="og:title" content="ARI Humanoid: Human-Like Gaze via Reinforcement Learning | Humanoids 2024">
  <meta property="og:description" content="Published at Humanoids 2024: Robust sim-to-real attention learning on ARI humanoid robot using PPO, domain randomisation, and zero-shot transfer. Socially aware gaze in dynamic HRI. Authors: Khashayar Ghamati, Abolfazl Zaraki, Farshid Amirabdollahian.">
  <meta property="og:image" content="https://ghamati.com/rlbam/ari-gaze-figure.jpg">
  <meta property="og:url" content="https://ghamati.com/rlbam/">
  <meta property="article:published_time" content="2024-11-22">
  <meta property="article:author" content="Khashayar Ghamati">
  <meta property="article:tag" content="reinforcement learning, ARI robot, human-robot interaction, sim-to-real, Humanoids 2024">

  <link rel="canonical" href="https://ghamati.com/rlbam/">

  <title>ARI Humanoid: Human-Like Gaze Behaviour with RL | Humanoids 2024 | Khashayar Ghamati</title>

  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "ScholarlyArticle",
      "headline": "ARI Humanoid Robot Learns Human-Like Gaze with RL: Robust Attention, Sim-to-Real & Zero-Shot Transfer",
      "author": [
        {
          "@type": "Person",
          "name": "Khashayar Ghamati",
          "affiliation": "University of Hertfordshire"
        },
        {
          "@type": "Person", 
          "name": "Abolfazl Zaraki",
          "affiliation": "University of Hertfordshire"
        },
        {
          "@type": "Person",
          "name": "Farshid Amirabdollahian", 
          "affiliation": "University of Hertfordshire"
        }
      ],
      "datePublished": "2024-11-22",
      "publisher": {
        "@type": "Organization",
        "name": "IEEE-RAS Humanoids Conference"
      },
      "about": [
        "humanoid robot",
        "reinforcement learning",
        "gaze behaviour",
        "attention modelling",
        "sim-to-real transfer",
        "human-robot interaction",
        "social robotics"
      ],
      "description": "This work demonstrates how the ARI humanoid robot acquires robust, human-like social gaze behaviour using deep reinforcement learning, domain randomisation, and zero-shot sim-to-real transfer for real-world HRI scenarios."
    }
  </script>

  <!-- Google Fonts -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap">
  <!-- External CSS for Icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer">

  <style>
    *, *::before, *::after {
      box-sizing: border-box;
    }
    
    body { font-family: 'Roboto', sans-serif; margin: 0; padding: 0; background-color: #f5f7fa; color: #333; line-height: 1.6; }
    .toolbar { background-color: #0077b6; padding: 1rem; position: sticky; top: 0; z-index: 1000; display: flex; justify-content: flex-start; box-shadow: 0 4px 10px rgba(0,0,0,0.1);}
    .toolbar a { color: white; text-decoration: none; font-size: 1.125rem; font-weight: 400; margin-right: 20px; transition: color 0.3s;}
    .toolbar a:hover { color: #94d2bd;}
    header { background-color: #0077b6; padding: 2rem 0; text-align: center; color: white; box-shadow: 0 2px 10px rgba(0,0,0,0.1);}
    header h1 { font-size: 2.5rem; margin: 0; font-weight: 700; }
    .container { max-width: 1200px; margin: 2rem auto; padding: 0 20px;}
    section h2 { font-size: 2rem; color: #0077b6; text-align: center; margin-bottom: 1.5rem; font-weight: 700;}
    section p { font-size: 1.125rem; line-height: 1.7; text-align: justify; margin-bottom: 2rem;}
    h3 { color: #333; font-size: 1.5rem; margin-bottom: 1rem;}
    ul { font-size: 1.125rem; line-height: 1.7;}
    .figure { text-align: center; margin: 20px 0; }
    .figure img { max-width: 50%; height: auto; border-radius: 8px; box-shadow: 0 4px 10px rgba(0,0,0,0.1); display: block; margin: 0 auto; }
    .video-container { display: flex; justify-content: center; margin-bottom: 2rem; }
    .video-container iframe { width: 560px; height: 315px; border-radius: 8px; box-shadow: 0 4px 10px rgba(0,0,0,0.1);}
    a.download-btn { display: inline-block; background-color: #0077b6; color: white; padding: 0.75rem 1.5rem; text-decoration: none; font-size: 1rem; border-radius: 5px; transition: background-color 0.3s; text-align: center; margin-top: 2rem; box-shadow: 0 4px 10px rgba(0,0,0,0.1);}
    a.download-btn:hover { background-color: #0096c7;}
    footer { text-align: center; padding: 2rem 0; background-color: #023e8a; color: white; margin-top: 3rem; box-shadow: 0 -2px 10px rgba(0,0,0,0.1);}
    @media (max-width: 768px) { header h1 { font-size: 2rem; } .figure img { max-width: 100%; } section p { font-size: 1rem; }
      .video-container iframe { width: 100%; height: 215px; } }
  </style>
</head>
<body>
<div class="toolbar">
  <a href="/"><i class="fas fa-home"></i> Home</a>
</div>

<header>
  <h1>ARI Humanoid Robot Learns Human-Like Gaze with RL: Robust Attention, Sim-to-Real & Zero-Shot Transfer</h1>
</header>

<div class="container">
  <section id="about">
    <h2>About the Paper</h2>
    <p>
      <b>Presented at IEEE-RAS Humanoids 2024</b>, this work demonstrates how the ARI humanoid robot acquires robust, human-like social gaze behaviour using <b>deep reinforcement learning</b> (RL), <b>domain randomisation</b>, and <b>zero-shot sim-to-real transfer</b>. We trained an attention model using <b>Proximal Policy Optimisation (PPO)</b> in simulation—then directly deployed it on ARI for real-world <b>human-robot interaction (HRI)</b> scenarios, achieving natural saliency-driven gaze and reliable adaptation to new people and activities. The approach tackles a core challenge in social robotics: robust transfer of RL policies from simulation to reality, enabling <b>real-time, context-sensitive attention</b> in complex, dynamic environments.
    </p>
    <p style="color: #0077b6;">
      <i class="fas fa-lightbulb"></i> <b>Highlight:</b> This is among the first works to combine <b>PPO, domain randomisation, and ZST</b> for real-world social attention learning on a commercial humanoid robot.
    </p>
  </section>

  <div class="figure">
    <img src="ari-gaze-figure.png" alt="ARI gaze behaviour RL: sim-to-real transfer and attention learning in social HRI (Humanoids 2024)" loading="eager" width="600" height="450">
    <p><b>Figure 1:</b> Workflow and example of the ARI robot dynamically allocating gaze between participants based on activity, proximity, and social cues, driven by RL policy trained in simulation.</p>
  </div>

  <!-- YouTube Video Embed -->
  <div class="video-container">
    <iframe src="https://www.youtube.com/embed/HI_fXSfgixk" title="ARI humanoid robot imitates human gaze behaviour using reinforcement learning in real-world environments" allowfullscreen></iframe>
  </div>

  <section id="context">
    <h2>Why Social Gaze Matters in Human-Robot Interaction</h2>
    <p>
      Gaze behaviour is a core channel for nonverbal communication and social presence in HRI. For a robot to be accepted as a social peer, it must identify salient participants and respond to dynamic social cues—just as humans do. Traditional hand-crafted gaze rules fail in unstructured or multi-party environments. Our RL-based model enables ARI to learn these nuanced behaviours directly from simulated experience, generalising to new people, positions, and activities in the real world.
    </p>
  </section>

  <section id="method">
    <h2>Methodology: Attention Model, PPO, and Sim-to-Real Transfer</h2>
    <ul>
      <li>
        <b>Randomisation & Generalisation:</b> We use domain randomisation to generate diverse training states: up to six people, random combinations of activities (speaking, entering, gestures, etc.), and proximity (intimate, social, public). This builds a robust policy that handles real-world variability.
      </li>
      <li>
        <b>Reward Function:</b> Our reward is based on an elicited attention (EA) score, adapted from human social attention research. The RL agent receives maximal reward for gazing at the participant performing the highest-priority, socially salient activity in closest proximity.
      </li>
      <li>
        <b>Training & Architecture:</b> PPO is trained in Gym and RayRL environments. The agent observes participant activities and distances; actions are gaze directions (at environment, objects, or participants 1-6). Hyperparameters are carefully tuned (entropy coefficient, gamma, learning rate) for fast convergence and robust exploration.
      </li>
      <li>
        <b>Sim-to-Real Pipeline:</b> After convergence, the RL policy is integrated with ARI’s ROS-based control system. Kinect V2 provides participant tracking; decisions are sent via ZeroMQ to the robot for real-time gaze control.
      </li>
      <li>
        <b>Zero-Shot Transfer:</b> No additional training or adaptation is required after simulation—the agent generalises directly to the real world using robust, diversified policy learning.
      </li>
    </ul>
  </section>

  <div class="figure">
    <img src="ari-gaze-demo.jpg" alt="Sim-to-real RL deployment: ARI attention targets and gaze allocation in live HRI" loading="lazy" width="700" height="500">
    <p><b>Figure 2:</b> Example from real-world trials. ARI dynamically allocates attention between environment, participant 1, and participant 2 in live triadic HRI, mirroring human-like saliency and prioritisation. RL agent selects gaze targets based on real-time EA scoring.</p>
  </div>

  <section id="results">
    <h2>Results & Evaluation (Humanoids 2024)</h2>
    <ul>
      <li>
        <b>Attention Allocation:</b> In real HRI, ARI correctly prioritised and gazed at the most socially salient participant (e.g., entering, speaking, hand gestures) in 94% of trials, and mimicked human-like low-level scanning when no one was present.
      </li>
      <li>
        <b>Robustness & Generalisation:</b> The model generalised to previously unseen combinations of activities, proximity, and participants, overcoming the sim-to-real gap without post-deployment retraining.
      </li>
      <li>
        <b>Sample Data:</b> During one scenario, ARI looked at participant 1 (high-priority activities) in 78.6% of frames, at participant 2 (lower priority) 15.9%, and at the environment when no participants were present—fully consistent with the designed EA reward and human reference data.
      </li>
      <li>
        <b>Quantitative Plots:</b> Mean episode reward steadily increased and stabilised after ~60k simulation episodes, showing effective and stable learning.</li>
      <li>
        <b>Proof-of-Concept:</b> The robot performed error-free saliency target selection in multi-person HRI, as validated by comparison with ideal action tables and ground-truth annotations.
      </li>
    </ul>
    <p>
      <i class="fas fa-robot"></i> <b>Impact:</b> This research marks a step change in how reinforcement learning and sim-to-real transfer can unlock robust, adaptive, and explainable social intelligence for commercial humanoid robots in the wild.
    </p>
  </section>

  <section id="challenges">
    <h2>Challenges & Future Directions</h2>
    <ul>
      <li>Sensor noise and real-world perception limits remain a bottleneck—domain randomisation helps but cannot eliminate all artefacts.</li>
      <li>Hyperparameter tuning (entropy, learning rate, PPO clip, gamma) is essential for robust and fast convergence; future work will integrate model-based RL and sensor fusion for even greater fidelity and adaptability.</li>
      <li>Extending this approach to multi-robot and crowded HRI, as well as integrating memory and dialogue, is a promising next step.</li>
    </ul>
  </section>

  <section id="open">
    <h2>Open Science, Paper, and Code</h2>
    <a href="https://ieeexplore.ieee.org/document/10769867" class="download-btn" target="_blank">Download Full Paper (IEEE Xplore)</a>
    <a href="#" class="download-btn" onclick="showCodeDialog(event)">Code</a>
    <p style="margin-top:1rem;font-size:1rem;color:#0077b6;">
      <i class="fas fa-info-circle"></i> Published at <b>IEEE-RAS Humanoids 2024</b>.<br>
      <a href="https://scholar.google.com/citations?user=zRG8t_oAAAAJ&hl=en" target="_blank" title="Google Scholar"><i class="fas fa-graduation-cap"></i> Google Scholar</a> |
      <a href="https://orcid.org/0009-0002-6416-3127" target="_blank" title="ORCID"><i class="fab fa-orcid"></i> ORCID</a>
    </p>
  </section>
</div>

<script>
  function showCodeDialog(event) {
    event.preventDefault();
    alert("To access the source code, please email us at: k.ghamati@herts.ac.uk");
  }
</script>

<footer>
  <p>&copy; 2024 Khashayar Ghamati, Abolfazl Zaraki, Farshid Amirabdollahian. All rights reserved.</p>
</footer>
</body>
</html>
