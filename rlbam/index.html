<!DOCTYPE html>
<html lang="en-GB">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="index, follow">

    <!-- DNS Prefetch and Preconnect for Performance -->
    <link rel="dns-prefetch" href="//fonts.googleapis.com">
    <link rel="dns-prefetch" href="//cdnjs.cloudflare.com">
    <link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>

    <!-- SEO Meta Tags -->
    <title>ARI Humanoid: RL-Based Gaze Behaviour | Humanoids 2024</title>
    <meta name="description" content="ARI humanoid robot learns human-like gaze via reinforcement learning with robust sim-to-real transfer. Published at IEEE Humanoids 2024.">
    <meta name="keywords" content="humanoid robot, ARI, reinforcement learning, gaze behaviour, attention modelling, sim-to-real transfer, domain randomisation, zero-shot transfer, PPO, human-robot interaction, HRI, social robotics, deep RL, generalisation, real-world robotics, University of Hertfordshire, Khashayar Ghamati, Humanoids 2024, social gaze, robot attention, saliency, ROS, Kinect, RL transfer learning, PPO tuning">
    <meta name="author" content="Khashayar Ghamati, Abolfazl Zaraki, Farshid Amirabdollahian">

    <!-- Open Graph -->
    <meta property="og:title" content="ARI Humanoid: RL-Based Gaze Behaviour | Humanoids 2024">
    <meta property="og:description" content="Robust sim-to-real attention learning on ARI humanoid robot using PPO, domain randomisation, and zero-shot transfer. Published at Humanoids 2024.">
    <meta property="og:image" content="https://ghamati.com/rlbam/ari-gaze-figure.png">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://ghamati.com/rlbam/">
    <meta property="og:site_name" content="Khashayar Ghamati - AI Research">
    <meta property="og:locale" content="en_GB">
    <meta property="article:published_time" content="2024-11-22">
    <meta property="article:author" content="Khashayar Ghamati">
    <meta property="article:tag" content="reinforcement learning, ARI robot, human-robot interaction, sim-to-real, Humanoids 2024">

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="ARI Humanoid: RL-Based Gaze Behaviour | Humanoids 2024">
    <meta name="twitter:description" content="Robust sim-to-real attention learning on ARI humanoid using PPO and zero-shot transfer. Humanoids 2024.">
    <meta name="twitter:image" content="https://ghamati.com/rlbam/ari-gaze-figure.png">

    <link rel="canonical" href="https://ghamati.com/rlbam/">

    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "ScholarlyArticle",
      "headline": "ARI Humanoid Robot Learns Human-Like Gaze with RL: Robust Attention, Sim-to-Real & Zero-Shot Transfer",
      "author": [
        {
          "@type": "Person",
          "name": "Khashayar Ghamati",
          "affiliation": "University of Hertfordshire"
        },
        {
          "@type": "Person",
          "name": "Abolfazl Zaraki",
          "affiliation": "University of Hertfordshire"
        },
        {
          "@type": "Person",
          "name": "Farshid Amirabdollahian",
          "affiliation": "University of Hertfordshire"
        }
      ],
      "datePublished": "2024-11-22",
      "publisher": {
        "@type": "Organization",
        "name": "IEEE-RAS Humanoids Conference"
      },
      "about": [
        "humanoid robot",
        "reinforcement learning",
        "gaze behaviour",
        "attention modelling",
        "sim-to-real transfer",
        "human-robot interaction",
        "social robotics"
      ],
      "description": "This work demonstrates how the ARI humanoid robot acquires robust, human-like social gaze behaviour using deep reinforcement learning, domain randomisation, and zero-shot sim-to-real transfer for real-world HRI scenarios."
    }
    </script>

    <!-- VideoObject Schema for Explanation Video -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "VideoObject",
      "name": "ARI Humanoid Robot Learns Human-Like Gaze with RL: Robust Attention, Sim-to-Real & Zero-Shot Transfer",
      "description": "Detailed explanation of how the ARI humanoid robot learns human-like gaze behaviour using deep reinforcement learning, PPO training, domain randomisation, and zero-shot sim-to-real transfer for social human-robot interaction. Published at IEEE Humanoids 2024.",
      "thumbnailUrl": "https://img.youtube.com/vi/KswWRoeFIj4/maxresdefault.jpg",
      "uploadDate": "2024-11-22",
      "duration": "PT8M",
      "contentUrl": "https://www.youtube.com/watch?v=KswWRoeFIj4",
      "embedUrl": "https://www.youtube.com/embed/KswWRoeFIj4",
      "publisher": {
        "@type": "Person",
        "name": "Khashayar Ghamati",
        "url": "https://ghamati.com"
      },
      "keywords": "ARI robot, reinforcement learning, gaze behaviour, PPO, sim-to-real transfer, humanoid robot, social robotics, attention model, human-robot interaction, IEEE Humanoids",
      "inLanguage": "en",
      "isFamilyFriendly": true
    }
    </script>

    <!-- VideoObject Schema for Demo Video -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "VideoObject",
      "name": "ARI Humanoid Robot Imitates Human Gaze Behaviour Using Reinforcement Learning in Real-World Environments",
      "description": "Live demonstration of the ARI humanoid robot performing human-like social gaze behaviour in real-world human-robot interaction. The robot dynamically allocates attention between participants using learned RL policy from simulation, showcasing successful zero-shot sim-to-real transfer.",
      "thumbnailUrl": "https://img.youtube.com/vi/HI_fXSfgixk/maxresdefault.jpg",
      "uploadDate": "2024-11-22",
      "duration": "PT3M",
      "contentUrl": "https://www.youtube.com/watch?v=HI_fXSfgixk",
      "embedUrl": "https://www.youtube.com/embed/HI_fXSfgixk",
      "publisher": {
        "@type": "Person",
        "name": "Khashayar Ghamati",
        "url": "https://ghamati.com"
      },
      "keywords": "ARI robot demo, gaze behaviour, humanoid robot, reinforcement learning, real-world HRI, social attention, robotic gaze, sim-to-real, IEEE Humanoids",
      "inLanguage": "en",
      "isFamilyFriendly": true
    }
    </script>

    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-HV1K905FF2"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-HV1K905FF2');
    </script>

    <!-- Google Fonts -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Playfair+Display:wght@400;600;700&display=swap">
    <!-- Material Design 3 Shared CSS -->
    <link rel="stylesheet" href="/css/material-design.css">
    <!-- External CSS for Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer">

    <style>
        .md-highlight-box .fas { margin-right: 0.5rem; }
    </style>
</head>
<body>

<a href="#main-content" class="md-skip-link">Skip to main content</a>

<!-- Top App Bar -->
<nav class="md-top-app-bar" id="top-bar">
    <div class="md-top-app-bar__nav">
        <a href="/"><i class="fas fa-home"></i> Home</a>
        <a href="#about"><i class="fas fa-file-alt"></i> About</a>
        <a href="#exp"><i class="fas fa-video"></i> Videos</a>
        <a href="#method"><i class="fas fa-cogs"></i> Method</a>
        <a href="#results"><i class="fas fa-chart-line"></i> Results</a>
        <a href="#open"><i class="fas fa-download"></i> Paper</a>
    </div>
</nav>

<!-- Hero Section -->
<header class="md-hero">
    <h1 class="md-hero__title">ARI Humanoid Robot Learns Human-Like Gaze with RL: Robust Attention, Sim-to-Real & Zero-Shot Transfer</h1>
</header>

<main id="main-content">
<div class="md-container">

    <section class="md-section md-card-elevated md-mt-32" id="about">
        <h2 class="md-headline-large">About the Paper</h2>
        <p class="md-body-large">
            <b>Presented at IEEE-RAS Humanoids 2024</b>, this work demonstrates how the ARI humanoid robot acquires robust, human-like social gaze behaviour using <b>deep reinforcement learning</b> (RL), <b>domain randomisation</b>, and <b>zero-shot sim-to-real transfer</b>. We trained an attention model using <b>Proximal Policy Optimisation (PPO)</b> in simulation — then directly deployed it on ARI for real-world <b>human-robot interaction (HRI)</b> scenarios, achieving natural saliency-driven gaze and reliable adaptation to new people and activities. The approach tackles a core challenge in social robotics: robust transfer of RL policies from simulation to reality, enabling <b>real-time, context-sensitive attention</b> in complex, dynamic environments.
        </p>

        <div class="md-highlight-box">
            <i class="fas fa-lightbulb"></i> <b>Highlight:</b> This is among the first works to combine <b>PPO, domain randomisation, and ZST</b> for real-world social attention learning on a commercial humanoid robot.
        </div>
    </section>

    <div class="md-figure md-mt-32">
        <img src="ari-gaze-figure.png" alt="ARI gaze behaviour RL: sim-to-real transfer and attention learning in social HRI (Humanoids 2024)" loading="eager" width="600" height="450">
        <p class="md-figure__caption"><b>Figure 1:</b> Workflow and example of the ARI robot dynamically allocating gaze between participants based on activity, proximity, and social cues, driven by RL policy trained in simulation.</p>
    </div>

    <section class="md-section md-card-elevated" id="exp">
        <h2 class="md-headline-large">ARI Robot Gaze Learning Explained</h2>
        <p class="md-body-large">
            This video explains how the ARI humanoid robot learns human-like gaze behaviour through deep reinforcement learning. Watch how we use Proximal Policy Optimisation (PPO), domain randomisation, and zero-shot sim-to-real transfer to train robust social attention models that generalise from simulation to real-world human-robot interaction without additional training.
        </p>
        <div class="md-card-outlined md-mt-24" style="text-align:center; padding:24px;">
            <iframe src="https://www.youtube.com/embed/KswWRoeFIj4"
                    title="ARI Humanoid Robot Learns Human-Like Gaze with RL: Robust Attention, Sim-to-Real & Zero-Shot Transfer"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen
                    loading="lazy"
                    style="width:100%; max-width:560px; height:315px; border:none; border-radius:var(--md-shape-md);"></iframe>
        </div>
    </section>

    <section class="md-section md-card-elevated" id="demo-video">
        <h2 class="md-headline-large">Live Demo: ARI Robot in Real-World HRI</h2>
        <p class="md-body-large">
            Watch the ARI humanoid robot in action, demonstrating learned gaze behaviour in real-world human-robot interaction scenarios. The robot dynamically allocates attention between multiple participants based on social cues, proximity, and activities — all driven by the reinforcement learning policy trained in simulation and transferred without additional training.
        </p>
        <div class="md-card-outlined md-mt-24" style="text-align:center; padding:24px;">
            <iframe src="https://www.youtube.com/embed/HI_fXSfgixk"
                    title="ARI humanoid robot imitates human gaze behaviour using reinforcement learning in real-world environments"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen
                    loading="lazy"
                    style="width:100%; max-width:560px; height:315px; border:none; border-radius:var(--md-shape-md);"></iframe>
        </div>
    </section>

    <section class="md-section md-card-elevated" id="context">
        <h2 class="md-headline-large">Why Social Gaze Matters in HRI</h2>
        <p class="md-body-large">
            Gaze behaviour is a core channel for nonverbal communication and social presence in HRI. For a robot to be accepted as a social peer, it must identify salient participants and respond to dynamic social cues — just as humans do. Traditional hand-crafted gaze rules fail in unstructured or multi-party environments. Our RL-based model enables ARI to learn these nuanced behaviours directly from simulated experience, generalising to new people, positions, and activities in the real world.
        </p>
    </section>

    <section class="md-section md-card-elevated" id="method">
        <h2 class="md-headline-large">Methodology</h2>
        <ul>
            <li class="md-body-large">
                <b>Randomisation & Generalisation:</b> We use domain randomisation to generate diverse training states: up to six people, random combinations of activities (speaking, entering, gestures, etc.), and proximity (intimate, social, public). This builds a robust policy that handles real-world variability.
            </li>
            <li class="md-body-large">
                <b>Reward Function:</b> Our reward is based on an elicited attention (EA) score, adapted from human social attention research. The RL agent receives maximal reward for gazing at the participant performing the highest-priority, socially salient activity in closest proximity.
            </li>
            <li class="md-body-large">
                <b>Training & Architecture:</b> PPO is trained in Gym and RayRL environments. The agent observes participant activities and distances; actions are gaze directions (at environment, objects, or participants 1-6). Hyperparameters are carefully tuned (entropy coefficient, gamma, learning rate) for fast convergence and robust exploration.
            </li>
            <li class="md-body-large">
                <b>Sim-to-Real Pipeline:</b> After convergence, the RL policy is integrated with ARI's ROS-based control system. Kinect V2 provides participant tracking; decisions are sent via ZeroMQ to the robot for real-time gaze control.
            </li>
            <li class="md-body-large">
                <b>Zero-Shot Transfer:</b> No additional training or adaptation is required after simulation — the agent generalises directly to the real world using robust, diversified policy learning.
            </li>
        </ul>
    </section>

    <div class="md-figure">
        <img src="ari-gaze-demo.jpg" alt="Sim-to-real RL deployment: ARI attention targets and gaze allocation in live HRI" loading="lazy" width="700" height="500">
        <p class="md-figure__caption"><b>Figure 2:</b> Example from real-world trials. ARI dynamically allocates attention between environment, participant 1, and participant 2 in live triadic HRI, mirroring human-like saliency and prioritisation. RL agent selects gaze targets based on real-time EA scoring.</p>
    </div>

    <section class="md-section md-card-elevated" id="results">
        <h2 class="md-headline-large">Results & Evaluation</h2>
        <ul>
            <li class="md-body-large">
                <b>Attention Allocation:</b> In real HRI, ARI correctly prioritised and gazed at the most socially salient participant (e.g., entering, speaking, hand gestures) in 94% of trials, and mimicked human-like low-level scanning when no one was present.
            </li>
            <li class="md-body-large">
                <b>Robustness & Generalisation:</b> The model generalised to previously unseen combinations of activities, proximity, and participants, overcoming the sim-to-real gap without post-deployment retraining.
            </li>
            <li class="md-body-large">
                <b>Sample Data:</b> During one scenario, ARI looked at participant 1 (high-priority activities) in 78.6% of frames, at participant 2 (lower priority) 15.9%, and at the environment when no participants were present — fully consistent with the designed EA reward and human reference data.
            </li>
            <li class="md-body-large">
                <b>Quantitative Plots:</b> Mean episode reward steadily increased and stabilised after ~60k simulation episodes, showing effective and stable learning.
            </li>
            <li class="md-body-large">
                <b>Proof-of-Concept:</b> The robot performed error-free saliency target selection in multi-person HRI, as validated by comparison with ideal action tables and ground-truth annotations.
            </li>
        </ul>

        <div class="md-highlight-box">
            <i class="fas fa-robot"></i> <b>Impact:</b> This research marks a step change in how reinforcement learning and sim-to-real transfer can unlock robust, adaptive, and explainable social intelligence for commercial humanoid robots in the wild.
        </div>
    </section>

    <section class="md-section md-card-elevated" id="challenges">
        <h2 class="md-headline-large">Challenges & Future Directions</h2>
        <ul>
            <li class="md-body-large">Sensor noise and real-world perception limits remain a bottleneck — domain randomisation helps but cannot eliminate all artefacts.</li>
            <li class="md-body-large">Hyperparameter tuning (entropy, learning rate, PPO clip, gamma) is essential for robust and fast convergence; future work will integrate model-based RL and sensor fusion for even greater fidelity and adaptability.</li>
            <li class="md-body-large">Extending this approach to multi-robot and crowded HRI, as well as integrating memory and dialogue, is a promising next step.</li>
        </ul>
    </section>

    <section class="md-section md-card-elevated" id="open">
        <h2 class="md-headline-large">Open Science, Paper, and Code</h2>
        <div style="text-align:center; padding:24px 0;">
            <a href="https://ieeexplore.ieee.org/document/10769867" class="md-btn-filled" target="_blank" rel="noopener">
                <i class="fas fa-download"></i> Download Full Paper (IEEE Xplore)
            </a>
            <a href="#" class="md-btn-outlined" onclick="showCodeDialog(event)">
                <i class="fas fa-code"></i> Access Code
            </a>
        </div>

        <div class="md-social-links" style="justify-content:center;">
            <a href="https://scholar.google.com/citations?user=zRG8t_oAAAAJ&hl=en" target="_blank" rel="noopener" title="Google Scholar">
                <i class="fas fa-graduation-cap"></i>
            </a>
            <a href="https://orcid.org/0009-0002-6416-3127" target="_blank" rel="noopener" title="ORCID">
                <i class="fab fa-orcid"></i>
            </a>
        </div>

        <p class="md-body-medium" style="margin-top:24px; text-align:center; color:var(--md-on-surface-variant);">
            <i class="fas fa-info-circle"></i> Published at <b>IEEE-RAS Humanoids 2024</b>.
        </p>
    </section>
</div>
</main>

<script>
    function showCodeDialog(event) {
        event.preventDefault();
        alert("To access the source code, please email us at: k.ghamati@herts.ac.uk");
    }

    // Top app bar scroll elevation
    window.addEventListener('scroll', function() {
        document.getElementById('top-bar').classList.toggle('md-scrolled', window.scrollY > 0);
    });
</script>

<footer style="background-color:var(--md-inverse-surface); color:var(--md-inverse-on-surface); text-align:center; padding:48px 24px;">
    <div class="md-container">
        <p class="md-body-large">&copy; 2024 Khashayar Ghamati, Abolfazl Zaraki, Farshid Amirabdollahian. All rights reserved.</p>
        <p class="md-body-medium" style="opacity:0.8; margin-top:8px;">Advancing social robotics through reinforcement learning</p>
    </div>
</footer>

</body>
</html>
