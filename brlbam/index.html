<!DOCTYPE html>
<html lang="en-GB">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <!-- DNS Prefetch and Preconnect for Performance -->
  <link rel="dns-prefetch" href="//fonts.googleapis.com">
  <link rel="dns-prefetch" href="//cdnjs.cloudflare.com">
  <link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  
  <meta name="description" content="Learning to Gaze: Bio-Inspired Attention Adaptation for Social Robots - A reinforcement learning framework with habituation mechanisms enabling real-time adaptation of robotic gaze behaviour. 93% accuracy in social HRI. By Khashayar Ghamati et al.">
  <meta name="keywords" content="social robotics, reinforcement learning, habituation mechanism, gaze behaviour, attention adaptation, bio-inspired AI, multi-objective Q-learning, human-robot interaction, ARI humanoid robot, real-time learning, exploration-exploitation, social cognition">
  <meta name="author" content="Khashayar Ghamati, Maryam Banitalebi Dehkordi, Hamed Rahimi Nohooji, Holger Voos, Farshid Amirabdollahian, Abolfazl Zaraki">

  <meta property="og:title" content="Learning to Gaze: Bio-Inspired Attention Adaptation for Social Robots">
  <meta property="og:description" content="Bio-inspired reinforcement learning framework enabling real-time adaptation of robotic gaze behaviour based on salient environmental and social cues. Achieves 93% accuracy in aligning with human attention patterns through habituation-guided multi-objective Q-learning.">
  <meta property="og:image" content="https://ghamati.com/brlbam/paper_img.jpeg">
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://ghamati.com/brlbam/">
  <meta property="og:site_name" content="Khashayar Ghamati - AI Research">
  <meta property="og:locale" content="en_GB">
  <meta property="article:published_time" content="2024-12-15">
  <meta property="article:author" content="Khashayar Ghamati">
  <meta property="article:tag" content="social robotics, reinforcement learning, habituation, attention adaptation, bio-inspired AI">

  <link rel="canonical" href="https://ghamati.com/brlbam/">

  <title>Learning to Gaze: Bio-Inspired Attention Adaptation for Social Robots | Khashayar Ghamati</title>

  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "ScholarlyArticle",
      "headline": "Learning to Gaze: Bio-Inspired Attention Adaptation for Social Robots",
      "author": [
        {
          "@type": "Person",
          "name": "Khashayar Ghamati",
          "affiliation": "University of Hertfordshire"
        },
        {
          "@type": "Person",
          "name": "Maryam Banitalebi Dehkordi",
          "affiliation": "University of Hertfordshire"
        },
        {
          "@type": "Person",
          "name": "Hamed Rahimi Nohooji",
          "affiliation": "University of Luxembourg"
        },
        {
          "@type": "Person",
          "name": "Holger Voos",
          "affiliation": "University of Luxembourg"
        },
        {
          "@type": "Person",
          "name": "Farshid Amirabdollahian",
          "affiliation": "University of Hertfordshire"
        },
        {
          "@type": "Person",
          "name": "Abolfazl Zaraki",
          "affiliation": "University of Hertfordshire"
        }
      ],
      "datePublished": "2024-12-15",
      "publisher": {
        "@type": "Organization",
        "name": "Under Review"
      },
      "about": [
        "social robotics",
        "reinforcement learning",
        "habituation mechanism",
        "gaze behaviour",
        "attention adaptation",
        "bio-inspired AI",
        "multi-objective learning",
        "human-robot interaction"
      ],
      "description": "A bio-inspired reinforcement learning framework that enables real-time adaptation of robotic gaze behaviour using habituation mechanisms to dynamically regulate exploration-exploitation trade-offs in social environments."
    }
  </script>

  <!-- Google Fonts -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Playfair+Display:wght@400;600;700&display=swap">
  <!-- External CSS for Icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer">

  <style>
    :root {
      /* Warm Colour Palette */
      --primary-warm: #D2691E;
      --primary-warm-light: #E6A85C;
      --primary-warm-dark: #B8860B;
      --secondary-warm: #CD853F;
      --accent-warm: #F4A460;
      --accent-gold: #DAA520;
      
      /* Neutral Warm Tones */
      --bg-warm: #FDF6E3;
      --bg-card: #FFFFFF;
      --text-primary: #2F1B14;
      --text-secondary: #5D4037;
      --text-muted: #8D6E63;
      
      /* Gradients */
      --gradient-warm: linear-gradient(135deg, var(--primary-warm) 0%, var(--secondary-warm) 100%);
      --gradient-gold: linear-gradient(135deg, var(--accent-gold) 0%, var(--accent-warm) 100%);
      
      /* Typography */
      --font-heading: 'Playfair Display', serif;
      --font-body: 'Inter', sans-serif;
      
      /* Shadows */
      --shadow-soft: 0 4px 20px rgba(210, 105, 30, 0.1);
      --shadow-medium: 0 8px 30px rgba(210, 105, 30, 0.15);
      --shadow-strong: 0 12px 40px rgba(210, 105, 30, 0.2);
    }

    *, *::before, *::after {
      box-sizing: border-box;
    }
    
    body {
      font-family: var(--font-body);
      margin: 0;
      padding: 0;
      background: var(--bg-warm);
      color: var(--text-primary);
      line-height: 1.7;
      font-size: 16px;
    }

    /* Toolbar Styling */
    .toolbar {
      background: var(--gradient-warm);
      padding: 1rem 0;
      position: sticky;
      top: 0;
      z-index: 1000;
      box-shadow: var(--shadow-medium);
      backdrop-filter: blur(10px);
    }

    .toolbar .container {
      display: flex;
      justify-content: flex-start;
      align-items: center;
      margin: 0;
      padding: 0 2rem;
    }

    .toolbar a {
      color: white;
      text-decoration: none;
      font-size: 1rem;
      font-weight: 500;
      padding: 0.5rem 1rem;
      border-radius: 25px;
      transition: all 0.3s ease;
      display: flex;
      align-items: center;
      gap: 0.5rem;
      margin-right: 0.5rem;
    }

    .toolbar a:hover {
      background: rgba(255, 255, 255, 0.2);
      transform: translateY(-2px);
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
    }

    /* Header Styling */
    header {
      background: var(--gradient-warm);
      padding: 4rem 0;
      text-align: center;
      color: white;
      position: relative;
      overflow: hidden;
    }

    header::before {
      content: '';
      position: absolute;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1000 300"><path fill="%23ffffff" fill-opacity="0.1" d="M0,96L48,112C96,128,192,160,288,170.7C384,181,480,171,576,154.7C672,139,768,117,864,117.3C960,117,1056,139,1152,149.3C1248,160,1344,160,1392,160L1440,160L1440,320L1392,320C1344,320,1248,320,1152,320C1056,320,960,320,864,320C768,320,672,320,576,320C480,320,384,320,288,320C192,320,96,320,48,320L0,320Z"/></svg>') repeat-x bottom;
      background-size: 1440px 160px;
    }

    header h1 {
      font-family: var(--font-heading);
      font-size: clamp(2rem, 5vw, 3.5rem);
      margin: 0;
      font-weight: 700;
      text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.1);
      line-height: 1.2;
      max-width: 1200px;
      margin: 0 auto;
      padding: 0 2rem;
    }

    header p {
      font-size: 1.2rem;
      margin-top: 1rem;
      opacity: 0.9;
      font-weight: 400;
    }

    /* Container */
    .container {
      max-width: 1200px;
      margin: 0 auto;
      padding: 0 2rem;
    }

    /* Main Content */
    main {
      padding: 4rem 0;
    }

    /* Section Styling */
    section {
      background: var(--bg-card);
      margin: 3rem 0;
      padding: 3rem;
      border-radius: 20px;
      box-shadow: var(--shadow-soft);
      position: relative;
      overflow: hidden;
    }

    section h2 {
      font-family: var(--font-heading);
      font-size: 2.5rem;
      color: var(--primary-warm);
      text-align: center;
      margin: 0 0 2rem 0;
      font-weight: 600;
      position: relative;
    }

    section h2::after {
      content: '';
      display: block;
      width: 80px;
      height: 4px;
      background: var(--gradient-gold);
      margin: 1rem auto;
      border-radius: 2px;
    }

    section h3 {
      font-family: var(--font-heading);
      color: var(--text-primary);
      font-size: 1.8rem;
      margin: 2rem 0 1rem 0;
      font-weight: 600;
    }

    section p {
      font-size: 1.1rem;
      line-height: 1.8;
      color: var(--text-secondary);
      margin-bottom: 1.5rem;
      text-align: left;
    }

    /* Enhanced Lists */
    ul {
      list-style: none;
      padding: 0;
    }

    ul li {
      position: relative;
      padding: 0.8rem 0 0.8rem 2rem;
      margin: 0.5rem 0;
      font-size: 1.1rem;
      line-height: 1.7;
      color: var(--text-secondary);
    }

    ul li::before {
      content: 'â†’';
      position: absolute;
      left: 0;
      color: var(--primary-warm);
      font-weight: bold;
      font-size: 1.2rem;
    }

    /* Figure Styling */
    .figure {
      text-align: center;
      margin: 3rem 0;
      background: var(--bg-card);
      padding: 2rem;
      border-radius: 15px;
      box-shadow: var(--shadow-soft);
    }

    .figure img {
      max-width: 100%;
      height: auto;
      border-radius: 12px;
      box-shadow: var(--shadow-medium);
      transition: transform 0.3s ease;
    }

    .figure img:hover {
      transform: scale(1.02);
    }

    .figure p {
      margin-top: 1.5rem;
      font-style: italic;
      color: var(--text-muted);
      font-size: 0.95rem;
      line-height: 1.6;
    }

    /* Authors Section */
    .authors {
      background: linear-gradient(135deg, rgba(210, 105, 30, 0.1) 0%, rgba(218, 165, 32, 0.1) 100%);
      padding: 2rem;
      border-radius: 15px;
      margin: 2rem 0;
      text-align: left;
      border-left: 4px solid var(--primary-warm);
      box-shadow: var(--shadow-soft);
    }

    .authors h3 {
      color: var(--primary-warm);
      margin-top: 0;
      display: flex;
      align-items: center;
      gap: 0.5rem;
    }

    .authors p {
      margin: 0.8rem 0;
      font-weight: 500;
    }

    /* Highlight Box */
    .highlight-box {
      background: linear-gradient(135deg, var(--accent-warm) 0%, var(--accent-gold) 100%);
      color: white;
      padding: 2rem;
      border-radius: 15px;
      margin: 2rem 0;
      box-shadow: var(--shadow-medium);
    }

    .highlight-box .fas {
      font-size: 1.2rem;
      margin-right: 0.5rem;
    }

    .highlight-box ul {
      margin: 1rem 0;
    }

    .highlight-box ul li {
      color: white;
    }

    .highlight-box ul li::before {
      color: rgba(255, 255, 255, 0.8);
    }

    /* Method Grid */
    .method-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
      gap: 2rem;
      margin: 2rem 0;
    }

    .method-card {
      background: var(--bg-card);
      padding: 2rem;
      border-radius: 15px;
      box-shadow: var(--shadow-soft);
      border-top: 4px solid var(--primary-warm);
      transition: transform 0.3s ease, box-shadow 0.3s ease;
    }

    .method-card:hover {
      transform: translateY(-5px);
      box-shadow: var(--shadow-medium);
    }

    .method-card h3 {
      color: var(--primary-warm);
      margin-top: 0;
      display: flex;
      align-items: center;
      gap: 0.5rem;
      font-size: 1.3rem;
    }

    /* Download Buttons */
    .download-btn {
      display: inline-block;
      background: var(--gradient-warm);
      color: white;
      padding: 1rem 2rem;
      margin: 0.5rem;
      text-decoration: none;
      font-size: 1.1rem;
      font-weight: 500;
      border-radius: 50px;
      transition: all 0.3s ease;
      box-shadow: var(--shadow-soft);
      position: relative;
      overflow: hidden;
    }

    .download-btn::before {
      content: '';
      position: absolute;
      top: 0;
      left: -100%;
      width: 100%;
      height: 100%;
      background: linear-gradient(90deg, transparent, rgba(255, 255, 255, 0.2), transparent);
      transition: left 0.5s;
    }

    .download-btn:hover {
      transform: translateY(-3px);
      box-shadow: var(--shadow-strong);
    }

    .download-btn:hover::before {
      left: 100%;
    }

    /* Footer */
    footer {
      background: var(--gradient-warm);
      color: white;
      text-align: center;
      padding: 3rem 0;
      margin-top: 4rem;
    }

    /* Social Links */
    .social-links {
      display: flex;
      justify-content: center;
      gap: 1rem;
      margin: 2rem 0;
    }

    .social-links a {
      color: var(--primary-warm);
      font-size: 1.5rem;
      transition: all 0.3s ease;
    }

    .social-links a:hover {
      color: var(--accent-gold);
      transform: translateY(-2px);
    }

    /* Responsive Design */
    @media (max-width: 768px) {
      .container {
        padding: 0 1rem;
      }
      
      section {
        padding: 2rem 1.5rem;
        margin: 2rem 0;
      }
      
      header {
        padding: 2rem 0;
      }
      
      section h2 {
        font-size: 2rem;
      }
      
      .figure {
        padding: 1rem;
      }
      
      .method-grid {
        grid-template-columns: 1fr;
      }
      
      .download-btn {
        display: block;
        margin: 1rem auto;
        max-width: 280px;
      }
      
      .toolbar {
        padding: 0.5rem 0;
      }
      
      .toolbar a {
        font-size: 0.9rem;
        padding: 0.4rem 0.8rem;
        margin-right: 0.3rem;
      }
    }

    /* Animations */
    @keyframes fadeInUp {
      from {
        opacity: 0;
        transform: translateY(30px);
      }
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }

    section {
      animation: fadeInUp 0.6s ease-out;
    }
  </style>
</head>
<body>
<div class="toolbar">
  <div class="container">
    <a href="/"><i class="fas fa-home"></i> Home</a>
    <a href="#abstract"><i class="fas fa-file-alt"></i> Abstract</a>
    <a href="#methodology"><i class="fas fa-cogs"></i> Method</a>
    <a href="#results"><i class="fas fa-chart-line"></i> Results</a>
  </div>
</div>

<header>
  <h1>Learning to Gaze: Bio-Inspired Attention Adaptation for Social Robots (Submitted)</h1>
  <p style="font-size: 1.2rem; margin-top: 1rem; opacity: 0.9;">A reinforcement learning framework with habituation mechanisms for real-time robotic attention adaptation </p>
</header>

<main>
<div class="container">
  <div class="authors">
    <h3><i class="fas fa-users"></i> Authors</h3>
<!--    <p><strong>Khashayar Ghamati</strong><sup>1,*</sup>, <strong>Maryam Banitalebi Dehkordi</strong><sup>1</sup>, <strong>Hamed Rahimi Nohooji</strong><sup>2</sup>, <strong>Holger Voos</strong><sup>2</sup>, <strong>Farshid Amirabdollahian</strong><sup>1</sup>, and <strong>Abolfazl Zaraki</strong><sup>1,*</sup></p>-->
    <p><strong>Khashayar Ghamati</strong><sup>1,*</sup>, et al.</p>
    <p style="font-size: 0.95rem; color: #666;">
      <sup>1</sup>School of Physics, Engineering and Computer Science (SPECS), Robotics Research Group, University of Hertfordshire, AL10 9AB, UK<br>
      <sup>2</sup>Automation Robotics Research Group, Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, Luxembourg<br>
      <sup>*</sup>Corresponding authors: k.ghamati@herts.ac.uk, a.zaraki@herts.ac.uk
    </p>
  </div>

  <section id="abstract">
    <h2>Abstract</h2>
    <div class="highlight-box">
      <p>
        <i class="fas fa-lightbulb" style="color: #4caf50;"></i> <strong>Key Innovation:</strong> This study presents the first bio-inspired reinforcement learning framework that enables real-time adaptation of robotic gaze behaviour using <strong>habituation mechanisms</strong> to dynamically regulate exploration-exploitation trade-offs in social environments.
      </p>
    </div>
    <p>
      Understanding and modelling human attention is essential for advancing autonomous systems able to operate in complex social environments. In this study, we present a bio-inspired reinforcement learning framework that enables real-time adaptation of robotic gaze behaviour based on salient environmental and social cues in human-robot interactions. The proposed system enables a social robot to engage in social interaction with humans and to identify the target points dynamically and gaze at the correct target point at the right moment.
    </p>
    <p>
      Central to our approach is a <strong>habituation mechanism</strong>, modelled on the human tendency to reduce sensitivity to repeated stimuli, which dynamically regulates the explorationâ€“exploitation trade-off in a multi-objective Q-learning setting. The proposed Reinforcement learning-based attention model was evaluated both in simulation and on a physical humanoid robot, demonstrating <strong>93% accuracy</strong> in aligning its gaze with human-identified points of interest across all interaction states.
    </p>
    <p>
      Our results provide evidence that incorporating biologically grounded mechanisms into learning algorithms enhances adaptive perception in embodied agents. This work offers generalisable insights into real-time decision-making and contributes towards the development of socially aware, cognitively plausible artificial agents.
    </p>
  </section>

  <div class="figure">
    <img src="paper_img.jpeg" alt="ARI humanoid robot in triadic social interaction at Robot House, University of Hertfordshire" loading="eager" width="700" height="500">
    <p><strong>Figure 1:</strong> A triadic social HRI between ARI humanoid robot and two study humans, demonstrating real-world deployment of the bio-inspired attention adaptation framework at the Robot House, University of Hertfordshire, UK.</p>
  </div>

  <section id="methodology">
    <h2>Methodology: Bio-Inspired Attention Framework</h2>

    <div class="method-grid">
      <div class="method-card">
        <h3><i class="fas fa-brain"></i> Habituation Mechanism</h3>
        <p>Inspired by biological attention regulation, our habituation mechanism dynamically adjusts exploration rates based on stimulus familiarity, preventing overfocus on redundant stimuli and enabling efficient reallocation to novel information.</p>
      </div>

      <div class="method-card">
        <h3><i class="fas fa-target"></i> Multi-Objective Q-Learning</h3>
        <p>Employs scalarisation-based approach to balance conflicting attentional demands including spatial proximity, activity salience, and person number, resulting in contextually adaptive gaze strategies.</p>
      </div>

      <div class="method-card">
        <h3><i class="fas fa-eye"></i> Elicited Attention Modeling</h3>
        <p>Reward function based on empirically-derived human attention data, incorporating social features and proxemic zones to ensure ecological validity in learned behaviours.</p>
      </div>

      <div class="method-card">
        <h3><i class="fas fa-users"></i> Real-Time Social Adaptation</h3>
        <p>Enables dynamic attention allocation in multiparty scenarios with up to 6 people, processing social cues, gestures, and proximity in real-time for human-like attention patterns.</p>
      </div>
    </div>

    <h3>Key Technical Innovations</h3>
    <ul>
      <li><strong>Habituation-Guided Exploration:</strong> Unlike conventional epsilon decay, our mechanism maps novelty sensitivity onto exploration parameters, maintaining contextual sensitivity while reducing redundancy</li>
      <li><strong>Scalarised Multi-Objective Learning:</strong> Weighted sum approach using normalised inverse standard deviation to balance 15 distinct reward objectives across social and environmental stimuli</li>
      <li><strong>Real-Time State Processing:</strong> Discrete action space with 8 gaze targets (6 people + environment + objects) enabling immediate response to dynamic social contexts</li>
      <li><strong>Empirical Validation:</strong> Ground truth established through eye-tracking studies with human participants observing social interactions</li>
    </ul>
  </section>

  <div class="figure">
    <img src="rlbam-architecture.png" alt="RLBAM framework architecture showing habituation mechanism and multi-objective learning" loading="lazy" width="800" height="600">
    <p><strong>Figure 2:</strong> The RLBAM framework architecture demonstrating the integration of habituation mechanisms with multi-objective Q-learning for adaptive robotic attention control.</p>
  </div>

  <section id="results">
    <h2>Results & Experimental Validation</h2>

    <div class="highlight-box">
      <p><i class="fas fa-trophy" style="color: #4caf50;"></i> <strong>Performance Highlights:</strong></p>
      <ul>
        <li><strong>93% accuracy</strong> in aligning gaze with human attention patterns across diverse interaction contexts</li>
        <li><strong>Real-time performance</strong> with immediate adaptation to new people, positions, and activities</li>
        <li><strong>Successful multiparty handling</strong> with dynamic priority adjustment based on activity salience and proximity</li>
        <li><strong>Zero-shot generalisation</strong> from simulation to real-world deployment without additional training</li>
      </ul>
    </div>

    <h3>Experimental Methodology</h3>
    <p>
      The framework was rigorously evaluated across multiple phases: simulation training in IsaacSim with diverse interaction scenarios, real-world deployment on the ARI humanoid robot, and comparative analysis against baseline attention models. Human attention data was collected using DIKABLIS eye-tracking system with 11 participants observing structured social interactions.
    </p>

    <h3>Key Findings</h3>
    <ul>
      <li><strong>Attention Allocation Accuracy:</strong> ARI correctly prioritised and gazed at the most socially salient participant in 94% of trials, demonstrating robust context-sensitive behaviour selection</li>
      <li><strong>Habituation Effectiveness:</strong> The bio-inspired exploration mechanism prevented local optima and maintained learning continuity, with reward convergence after ~2,500 episodes</li>
      <li><strong>Multiparty Interaction Success:</strong> Successfully distinguished between multiple salient points based on varying activity levels and engagement patterns of multiple humans</li>
      <li><strong>Real-World Robustness:</strong> Maintained performance despite environmental factors including occlusions, tracking latency, and sensor noise</li>
    </ul>

    <p>
      The 7% performance deviation primarily resulted from practical deployment factors rather than policy failures, demonstrating the framework's robustness in naturalistic environments. The system operated in real-time with action selection guided by the learned policy continuously updated in response to perceived states.
    </p>
  </section>

  <div class="figure">
    <img src="training-dynamics.png" alt="Training dynamics showing habituation-guided learning progression" loading="lazy" width="800" height="600">
    <p><strong>Figure 3:</strong> Training dynamics and behavioural analysis showing habituation-guided exploration, reward progression, and Pareto front evolution across 5,000 episodes.</p>
  </div>

  <section id="impact">
    <h2>Impact & Future Directions</h2>
    <p>
      This research marks a significant step towards cognitively plausible, real-time attention behaviour in social agents designed for unstructured, multiparty environments. By moving beyond static or pre-programmed gaze control, the proposed model provides a pathway towards more human-like social intelligence in robotics.
    </p>

    <h3>Broader Implications</h3>
    <ul>
      <li><strong>Social Robotics:</strong> Enables more natural and engaging human-robot interactions in care, education, and entertainment domains</li>
      <li><strong>Cognitive Science:</strong> Provides computational validation of biological attention mechanisms and habituation theories</li>
      <li><strong>AI/ML:</strong> Demonstrates the value of bio-inspired approaches in solving complex multi-objective learning problems</li>
      <li><strong>HCI:</strong> Offers principles for designing more socially aware and responsive artificial agents</li>
    </ul>

    <h3>Future Research Directions</h3>
    <p>
      Future work will explore continual learning capabilities, semantic scene parsing integration, and large-scale deployment across diverse demographic groups. The framework's principles may extend to other embodied AI applications requiring dynamic attention allocation in social contexts.
    </p>
  </section>

  <section id="access">
    <h2>Paper Access & Resources</h2>
    <div style="text-align: center; padding: 2rem 0;">
      <a href="#" class="download-btn" onclick="showPaperDialog(event)">
        <i class="fas fa-download"></i> Download Full Paper
      </a>
      <a href="#" class="download-btn" onclick="showCodeDialog(event)">
        <i class="fas fa-code"></i> Source Code & Data
      </a>
      <a href="#" class="download-btn" onclick="showVideoDialog(event)">
        <i class="fas fa-video"></i> Supplementary Materials
      </a>
    </div>

    <div class="social-links">
      <a href="https://scholar.google.com/citations?user=zRG8t_oAAAAJ&hl=en" target="_blank" rel="noopener" title="Google Scholar">
        <i class="fas fa-graduation-cap"></i>
      </a>
      <a href="https://orcid.org/0009-0002-6416-3127" target="_blank" rel="noopener" title="ORCID">
        <i class="fab fa-orcid"></i>
      </a>
      <a href="https://github.com/kghamati" target="_blank" rel="noopener" title="GitHub">
        <i class="fab fa-github"></i>
      </a>
    </div>

    <p style="margin-top:2rem;font-size:1rem;color:var(--text-muted);text-align:center;">
      <i class="fas fa-info-circle"></i> <strong>Publication Status:</strong> Under Review
    </p>
  </section>
</div>
</main>

<script>
  function showPaperDialog(event) {
    event.preventDefault();
    alert("Full paper will be available upon publication. For early access, please contact: k.ghamati@herts.ac.uk");
  }

  function showCodeDialog(event) {
    event.preventDefault();
    alert("For access inquiries: k.ghamati@herts.ac.uk");
  }

  function showVideoDialog(event) {
    event.preventDefault();
    alert("Contact authors for preview access: k.ghamati@herts.ac.uk");
  }
</script>

<footer>
  <div class="container">
    <p>&copy; 2025 Khashayar Ghamati. All rights reserved.</p>
    <p style="margin-top: 0.5rem; font-size: 0.9rem; opacity: 0.8;">University of Hertfordshire & University of Luxembourg</p>
  </div>
</footer>
</body>
</html>