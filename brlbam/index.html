<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Learning to Gaze: Bio-Inspired Attention Adaptation for Social Robots - A reinforcement learning framework with habituation mechanisms enabling real-time adaptation of robotic gaze behaviour. 93% accuracy in social HRI. By Khashayar Ghamati et al.">
  <meta name="keywords" content="social robotics, reinforcement learning, habituation mechanism, gaze behaviour, attention adaptation, bio-inspired AI, multi-objective Q-learning, human-robot interaction, ARI humanoid robot, real-time learning, exploration-exploitation, social cognition">
  <meta name="author" content="Khashayar Ghamati, Maryam Banitalebi Dehkordi, Hamed Rahimi Nohooji, Holger Voos, Farshid Amirabdollahian, Abolfazl Zaraki">

  <meta property="og:title" content="Learning to Gaze: Bio-Inspired Attention Adaptation for Social Robots">
  <meta property="og:description" content="Bio-inspired reinforcement learning framework enabling real-time adaptation of robotic gaze behaviour based on salient environmental and social cues. Achieves 93% accuracy in aligning with human attention patterns through habituation-guided multi-objective Q-learning.">
  <meta property="og:image" content="https://ghamati.com/bio-gaze/bio-gaze-framework.jpg">
  <meta property="og:url" content="https://ghamati.com/bio-gaze/">
  <meta property="article:published_time" content="2024-12-15">
  <meta property="article:author" content="Khashayar Ghamati">
  <meta property="article:tag" content="social robotics, reinforcement learning, habituation, attention adaptation, bio-inspired AI">

  <link rel="canonical" href="https://ghamati.com/bio-gaze/">

  <title>Learning to Gaze: Bio-Inspired Attention Adaptation for Social Robots | Khashayar Ghamati</title>

  <!-- Google Fonts -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap">
  <!-- External CSS for Icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

  <style>
    body { font-family: 'Roboto', sans-serif; margin: 0; padding: 0; background-color: #f5f7fa; color: #333; }
    .toolbar { background-color: #2e7d32; padding: 1rem; position: sticky; top: 0; z-index: 1000; display: flex; justify-content: flex-start; box-shadow: 0 4px 10px rgba(0,0,0,0.1);}
    .toolbar a { color: white; text-decoration: none; font-size: 1.125rem; font-weight: 400; margin-right: 20px; transition: color 0.3s;}
    .toolbar a:hover { color: #a5d6a7;}
    header { background: linear-gradient(135deg, #2e7d32, #4caf50); padding: 2rem 0; text-align: center; color: white; box-shadow: 0 2px 10px rgba(0,0,0,0.1);}
    header h1 { font-size: 2.5rem; margin: 0; font-weight: 700; line-height: 1.2; }
    .container { max-width: 1200px; margin: 2rem auto; padding: 0 20px;}
    section h2 { font-size: 2rem; color: #2e7d32; text-align: center; margin-bottom: 1.5rem; font-weight: 700;}
    section p { font-size: 1.125rem; line-height: 1.6; text-align: justify; margin-bottom: 2rem;}
    h3 { color: #333; font-size: 1.5rem; margin-bottom: 1rem;}
    ul { font-size: 1.125rem; line-height: 1.6;}
    .figure { text-align: center; margin: 30px 0; }
    .figure img { max-width: 70%; height: auto; border-radius: 12px; box-shadow: 0 6px 20px rgba(0,0,0,0.15);}
    .video-container { display: flex; justify-content: center; margin-bottom: 2rem; }
    .video-container iframe { width: 560px; height: 315px; border-radius: 8px; box-shadow: 0 4px 10px rgba(0,0,0,0.1);}
    a.download-btn { display: inline-block; background-color: #2e7d32; color: white; padding: 0.75rem 1.5rem; text-decoration: none; font-size: 1rem; border-radius: 5px; transition: background-color 0.3s; text-align: center; margin: 10px 10px 10px 0; box-shadow: 0 4px 10px rgba(0,0,0,0.1);}
    a.download-btn:hover { background-color: #1b5e20;}
    .highlight-box { background: linear-gradient(135deg, #e8f5e8, #f1f8f1); border-left: 4px solid #4caf50; padding: 1.5rem; margin: 2rem 0; border-radius: 8px; }
    .authors { background-color: #ffffff; padding: 1.5rem; border-radius: 10px; margin: 2rem 0; box-shadow: 0 2px 10px rgba(0,0,0,0.05); }
    .method-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 2rem; margin: 2rem 0; }
    .method-card { background: white; padding: 1.5rem; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); border-top: 3px solid #4caf50; }
    footer { text-align: center; padding: 2rem 0; background-color: #1b5e20; color: white; margin-top: 3rem; box-shadow: 0 -2px 10px rgba(0,0,0,0.1);}
    @media (max-width: 768px) {
      header h1 { font-size: 2rem; }
      .figure img { max-width: 100%; }
      section p { font-size: 1rem; }
      .video-container iframe { width: 100%; height: 215px; }
      .method-grid { grid-template-columns: 1fr; }
    }
  </style>
</head>
<body>
<div class="toolbar">
  <a href="/"><i class="fas fa-home"></i> Home</a>
  <a href="#abstract"><i class="fas fa-file-alt"></i> Abstract</a>
  <a href="#methodology"><i class="fas fa-cogs"></i> Method</a>
  <a href="#results"><i class="fas fa-chart-line"></i> Results</a>
</div>

<header>
  <h1>Learning to Gaze: Bio-Inspired Attention Adaptation for Social Robots (Submitted)</h1>
  <p style="font-size: 1.2rem; margin-top: 1rem; opacity: 0.9;">A reinforcement learning framework with habituation mechanisms for real-time robotic attention adaptation </p>
</header>

<div class="container">
  <div class="authors">
    <h3><i class="fas fa-users"></i> Authors</h3>
    <p><strong>Khashayar Ghamati</strong><sup>1,*</sup>, <strong>Maryam Banitalebi Dehkordi</strong><sup>1</sup>, <strong>Hamed Rahimi Nohooji</strong><sup>2</sup>, <strong>Holger Voos</strong><sup>2</sup>, <strong>Farshid Amirabdollahian</strong><sup>1</sup>, and <strong>Abolfazl Zaraki</strong><sup>1,*</sup></p>
    <p style="font-size: 0.95rem; color: #666;">
      <sup>1</sup>School of Physics, Engineering and Computer Science (SPECS), Robotics Research Group, University of Hertfordshire, AL10 9AB, UK<br>
      <sup>2</sup>Automation Robotics Research Group, Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, Luxembourg<br>
      <sup>*</sup>Corresponding authors: k.ghamati@herts.ac.uk, a.zaraki@herts.ac.uk
    </p>
  </div>

  <section id="abstract">
    <h2>Abstract</h2>
    <div class="highlight-box">
      <p>
        <i class="fas fa-lightbulb" style="color: #4caf50;"></i> <strong>Key Innovation:</strong> This study presents the first bio-inspired reinforcement learning framework that enables real-time adaptation of robotic gaze behaviour using <strong>habituation mechanisms</strong> to dynamically regulate exploration-exploitation trade-offs in social environments.
      </p>
    </div>
    <p>
      Understanding and modelling human attention is essential for advancing autonomous systems able to operate in complex social environments. In this study, we present a bio-inspired reinforcement learning framework that enables real-time adaptation of robotic gaze behaviour based on salient environmental and social cues in human-robot interactions. The proposed system enables a social robot to engage in social interaction with humans and to identify the target points dynamically and gaze at the correct target point at the right moment.
    </p>
    <p>
      Central to our approach is a <strong>habituation mechanism</strong>, modelled on the human tendency to reduce sensitivity to repeated stimuli, which dynamically regulates the explorationâ€“exploitation trade-off in a multi-objective Q-learning setting. The proposed Reinforcement learning-based attention model was evaluated both in simulation and on a physical humanoid robot, demonstrating <strong>93% accuracy</strong> in aligning its gaze with human-identified points of interest across all interaction states.
    </p>
    <p>
      Our results provide evidence that incorporating biologically grounded mechanisms into learning algorithms enhances adaptive perception in embodied agents. This work offers generalisable insights into real-time decision-making and contributes towards the development of socially aware, cognitively plausible artificial agents.
    </p>
  </section>

  <div class="figure">
    <img src="paper_img.jpeg" alt="ARI humanoid robot in triadic social interaction at Robot House, University of Hertfordshire">
    <p><strong>Figure 1:</strong> A triadic social HRI between ARI humanoid robot and two study humans, demonstrating real-world deployment of the bio-inspired attention adaptation framework at the Robot House, University of Hertfordshire, UK.</p>
  </div>

  <section id="methodology">
    <h2>Methodology: Bio-Inspired Attention Framework</h2>

    <div class="method-grid">
      <div class="method-card">
        <h3><i class="fas fa-brain"></i> Habituation Mechanism</h3>
        <p>Inspired by biological attention regulation, our habituation mechanism dynamically adjusts exploration rates based on stimulus familiarity, preventing overfocus on redundant stimuli and enabling efficient reallocation to novel information.</p>
      </div>

      <div class="method-card">
        <h3><i class="fas fa-target"></i> Multi-Objective Q-Learning</h3>
        <p>Employs scalarisation-based approach to balance conflicting attentional demands including spatial proximity, activity salience, and person number, resulting in contextually adaptive gaze strategies.</p>
      </div>

      <div class="method-card">
        <h3><i class="fas fa-eye"></i> Elicited Attention Modeling</h3>
        <p>Reward function based on empirically-derived human attention data, incorporating social features and proxemic zones to ensure ecological validity in learned behaviours.</p>
      </div>

      <div class="method-card">
        <h3><i class="fas fa-users"></i> Real-Time Social Adaptation</h3>
        <p>Enables dynamic attention allocation in multiparty scenarios with up to 6 people, processing social cues, gestures, and proximity in real-time for human-like attention patterns.</p>
      </div>
    </div>

    <h3>Key Technical Innovations</h3>
    <ul>
      <li><strong>Habituation-Guided Exploration:</strong> Unlike conventional epsilon decay, our mechanism maps novelty sensitivity onto exploration parameters, maintaining contextual sensitivity while reducing redundancy</li>
      <li><strong>Scalarised Multi-Objective Learning:</strong> Weighted sum approach using normalised inverse standard deviation to balance 15 distinct reward objectives across social and environmental stimuli</li>
      <li><strong>Real-Time State Processing:</strong> Discrete action space with 8 gaze targets (6 people + environment + objects) enabling immediate response to dynamic social contexts</li>
      <li><strong>Empirical Validation:</strong> Ground truth established through eye-tracking studies with human participants observing social interactions</li>
    </ul>
  </section>

  <div class="figure">
    <img src="rlbam-architecture.png" alt="RLBAM framework architecture showing habituation mechanism and multi-objective learning">
    <p><strong>Figure 2:</strong> The RLBAM framework architecture demonstrating the integration of habituation mechanisms with multi-objective Q-learning for adaptive robotic attention control.</p>
  </div>

  <section id="results">
    <h2>Results & Experimental Validation</h2>

    <div class="highlight-box">
      <p><i class="fas fa-trophy" style="color: #4caf50;"></i> <strong>Performance Highlights:</strong></p>
      <ul>
        <li><strong>93% accuracy</strong> in aligning gaze with human attention patterns across diverse interaction contexts</li>
        <li><strong>Real-time performance</strong> with immediate adaptation to new people, positions, and activities</li>
        <li><strong>Successful multiparty handling</strong> with dynamic priority adjustment based on activity salience and proximity</li>
        <li><strong>Zero-shot generalisation</strong> from simulation to real-world deployment without additional training</li>
      </ul>
    </div>

    <h3>Experimental Methodology</h3>
    <p>
      The framework was rigorously evaluated across multiple phases: simulation training in IsaacSim with diverse interaction scenarios, real-world deployment on the ARI humanoid robot, and comparative analysis against baseline attention models. Human attention data was collected using DIKABLIS eye-tracking system with 11 participants observing structured social interactions.
    </p>

    <h3>Key Findings</h3>
    <ul>
      <li><strong>Attention Allocation Accuracy:</strong> ARI correctly prioritised and gazed at the most socially salient participant in 94% of trials, demonstrating robust context-sensitive behaviour selection</li>
      <li><strong>Habituation Effectiveness:</strong> The bio-inspired exploration mechanism prevented local optima and maintained learning continuity, with reward convergence after ~2,500 episodes</li>
      <li><strong>Multiparty Interaction Success:</strong> Successfully distinguished between multiple salient points based on varying activity levels and engagement patterns of multiple humans</li>
      <li><strong>Real-World Robustness:</strong> Maintained performance despite environmental factors including occlusions, tracking latency, and sensor noise</li>
    </ul>

    <p>
      The 7% performance deviation primarily resulted from practical deployment factors rather than policy failures, demonstrating the framework's robustness in naturalistic environments. The system operated in real-time with action selection guided by the learned policy continuously updated in response to perceived states.
    </p>
  </section>

  <div class="figure">
    <img src="training-dynamics.png" alt="Training dynamics showing habituation-guided learning progression">
    <p><strong>Figure 3:</strong> Training dynamics and behavioural analysis showing habituation-guided exploration, reward progression, and Pareto front evolution across 5,000 episodes.</p>
  </div>

  <section id="impact">
    <h2>Impact & Future Directions</h2>
    <p>
      This research marks a significant step towards cognitively plausible, real-time attention behaviour in social agents designed for unstructured, multiparty environments. By moving beyond static or pre-programmed gaze control, the proposed model provides a pathway towards more human-like social intelligence in robotics.
    </p>

    <h3>Broader Implications</h3>
    <ul>
      <li><strong>Social Robotics:</strong> Enables more natural and engaging human-robot interactions in care, education, and entertainment domains</li>
      <li><strong>Cognitive Science:</strong> Provides computational validation of biological attention mechanisms and habituation theories</li>
      <li><strong>AI/ML:</strong> Demonstrates the value of bio-inspired approaches in solving complex multi-objective learning problems</li>
      <li><strong>HCI:</strong> Offers principles for designing more socially aware and responsive artificial agents</li>
    </ul>

    <h3>Future Research Directions</h3>
    <p>
      Future work will explore continual learning capabilities, semantic scene parsing integration, and large-scale deployment across diverse demographic groups. The framework's principles may extend to other embodied AI applications requiring dynamic attention allocation in social contexts.
    </p>
  </section>

  <section id="access">
    <h2>Paper Access & Resources</h2>
    <a href="#" class="download-btn" onclick="showPaperDialog(event)">Download Full Paper</a>
    <a href="#" class="download-btn" onclick="showCodeDialog(event)">Source Code & Data</a>
    <a href="#" class="download-btn" onclick="showVideoDialog(event)">Supplementary Materials</a>

    <p style="margin-top:1.5rem;font-size:1rem;color:#2e7d32;">
      <i class="fas fa-info-circle"></i> <strong>Publication Status:</strong> Under Review<br>
      <a href="https://scholar.google.com/citations?user=zRG8t_oAAAAJ&hl=en" target="_blank" title="Google Scholar"><i class="fas fa-graduation-cap"></i> Google Scholar</a> |
      <a href="https://orcid.org/0009-0002-6416-3127" target="_blank" title="ORCID"><i class="fab fa-orcid"></i> ORCID</a> |
      <a href="https://github.com/kghamati" target="_blank" title="GitHub"><i class="fab fa-github"></i> GitHub</a>
    </p>

<!--    <div class="highlight-box" style="margin-top: 2rem;">-->
<!--      <p><i class="fas fa-quote-left"></i> <strong>Citation:</strong></p>-->
<!--      <p style="font-family: monospace; background: #f9f9f9; padding: 1rem; border-radius: 5px; font-size: 0.9rem;">-->
<!--        Ghamati, K., Banitalebi Dehkordi, M., Rahimi Nohooji, H., Voos, H., Amirabdollahian, F., & Zaraki, A. (2024). Learning to Gaze: Bio-Inspired Attention Adaptation for Social Robots. <em>Under Review</em>.-->
<!--      </p>-->
<!--    </div>-->
  </section>
</div>

<script>
  function showPaperDialog(event) {
    event.preventDefault();
    alert("Full paper will be available upon publication. For early access, please contact: k.ghamati@herts.ac.uk");
  }

  function showCodeDialog(event) {
    event.preventDefault();
    alert("For access inquiries: k.ghamati@herts.ac.uk");
  }

  function showVideoDialog(event) {
    event.preventDefault();
    alert("Contact authors for preview access: k.ghamati@herts.ac.uk");
  }
</script>

<footer>
  <p>&copy; 2024 Khashayar Ghamati, Maryam Banitalebi Dehkordi, Hamed Rahimi Nohooji, Holger Voos, Farshid Amirabdollahian, Abolfazl Zaraki. All rights reserved.</p>
  <p style="margin-top: 0.5rem; font-size: 0.9rem; opacity: 0.8;">University of Hertfordshire & University of Luxembourg</p>
</footer>
</body>
</html>