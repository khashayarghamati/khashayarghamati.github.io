<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <!-- DNS Prefetch and Preconnect for Performance -->
  <link rel="dns-prefetch" href="//fonts.googleapis.com">
  <link rel="dns-prefetch" href="//cdnjs.cloudflare.com">
  <link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  
  <!-- SEO Meta Tags -->
  <meta name="description" content="Which AI Sees Like Us? Investigating the cognitive plausibility of language and vision models via eye-tracking in human-robot interaction. A comprehensive study using VLMs and LLMs to model human visual attention patterns.">
  <meta name="keywords" content="human attention modeling, LLMs, VLMs, human-robot interaction, human-computer interaction, personalised large language models, adaptive AI systems, AI agent, short-term memory, eye-tracking, cognitive plausibility, visual attention, saliency detection, Khashayar Ghamati">

  <!-- Open Graph for social and richer search results -->
  <meta property="og:title" content="Which AI Sees Like Us? Investigating Cognitive Plausibility in AI Vision Models">
  <meta property="og:description" content="A novel study examining how well large language models and vision-language models replicate human visual attention patterns in human-robot interaction scenarios through eye-tracking analysis.">
  <meta property="og:image" content="https://ghamati.com/seelikeus/LLM_comprasion.png">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://ghamati.com/seelikeus/">

  <!-- Twitter Card -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Which AI Sees Like Us? Cognitive Plausibility in AI Vision Models">
  <meta name="twitter:description" content="Eye-tracking study comparing LLMs and VLMs with human visual attention patterns in HRI scenarios.">
  <meta name="twitter:image" content="https://ghamati.com/seelikeus/LLM_comprasion.png">

  <link rel="canonical" href="https://ghamati.com/seelikeus/">

  <title>Which AI Sees Like Us? Investigating Cognitive Plausibility of Vision Models | Khashayar Ghamati</title>

  <!-- Google Fonts -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap">

  <!-- External CSS for Icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer">

  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-HV1K905FF2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-HV1K905FF2');
  </script>

  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "ScholarlyArticle",
      "headline": "Which AI Sees Like Us? Investigating the Cognitive Plausibility of Language and Vision Models via Eye-Tracking in Human-Robot Interaction",
      "author": [
        {
          "@type": "Person",
          "name": "Khashayar Ghamati",
          "affiliation": "University of Hertfordshire"
        },
        {
          "@type": "Person",
          "name": "Maryam Banitalebi Dehkordi",
          "affiliation": "University of Hertfordshire"
        },
        {
          "@type": "Person",
          "name": "Abolfazl Zaraki",
          "affiliation": "University of Hertfordshire"
        }
      ],
      "datePublished": "2025-07-18",
      "publisher": {
        "@type": "Organization",
        "name": "Journal Not Specified"
      },
      "about": [
        "human attention modeling",
        "LLMs",
        "VLMs",
        "human-robot interaction",
        "eye-tracking",
        "cognitive plausibility",
        "visual attention",
        "AI agents"
      ],
      "description": "This study investigates the cognitive plausibility of large language models and vision-language models by comparing their visual attention predictions with human eye-tracking data in human-robot interaction scenarios."
    }
  </script>

  <style>
    *, *::before, *::after {
      box-sizing: border-box;
    }
    
    body {
      font-family: 'Roboto', sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f5f7fa;
      color: #333;
      line-height: 1.6;
    }

    /* Toolbar Styling */
    .toolbar {
      background-color: #0077b6;
      padding: 1rem;
      position: sticky;
      top: 0;
      z-index: 1000;
      display: flex;
      justify-content: flex-start;
      box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
    }

    .toolbar a {
      color: white;
      text-decoration: none;
      font-size: 1.125rem;
      font-weight: 400;
      margin-right: 20px;
      transition: color 0.3s;
    }

    .toolbar a:hover {
      color: #94d2bd;
    }

    header {
      background-color: #0077b6;
      padding: 2rem 0;
      text-align: center;
      color: white;
      box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
    }

    header h1 {
      font-size: 2.5rem;
      margin: 0;
      font-weight: 700;
    }

    .container {
      max-width: 1200px;
      margin: 2rem auto;
      padding: 0 20px;
    }

    section h2 {
      font-size: 2rem;
      color: #0077b6;
      text-align: center;
      margin-bottom: 1.5rem;
      font-weight: 700;
    }

    section p {
      font-size: 1.125rem;
      line-height: 1.7;
      text-align: justify;
      margin-bottom: 2rem;
    }

    h3 {
      color: #333;
      font-size: 1.5rem;
      margin-bottom: 1rem;
    }

    ul {
      font-size: 1.125rem;
      line-height: 1.7;
    }

    .figure {
      text-align: center;
      margin: 20px 0;
    }

    .figure img {
      max-width: 100%;
      width: 100%;
      height: auto;
      display: block;
      margin: 0 auto;
    }
    
    @media (min-width: 768px) {
      .figure img {
        max-width: 70%;
      }
      .figure-large {
        max-width: 90% !important;
      }
    }

    /* Download button */
    a.download-btn {
      display: inline-block;
      background-color: #0077b6;
      color: white;
      padding: 0.75rem 1.5rem;
      text-decoration: none;
      font-size: 1rem;
      border-radius: 5px;
      transition: background-color 0.3s;
      text-align: center;
      margin-top: 2rem;
      box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
    }

    a.download-btn:hover {
      background-color: #0096c7;
    }

    footer {
      text-align: center;
      padding: 2rem 0;
      background-color: #023e8a;
      color: white;
      margin-top: 3rem;
      box-shadow: 0 -2px 10px rgba(0, 0, 0, 0.1);
    }

    .authors {
      font-style: italic;
      margin-bottom: 1rem;
      text-align: center;
    }

    .highlight-box {
      background-color: #e8f4f8;
      border-left: 4px solid #0077b6;
      padding: 1rem;
      margin: 1.5rem 0;
      border-radius: 5px;
    }

  </style>
</head>
<body>

<!-- Toolbar with Home Button -->
<div class="toolbar">
  <a href="/"><i class="fas fa-home"></i> Home</a>
</div>

<header>
  <h1>Which AI Sees Like Us? Investigating Cognitive Plausibility of Vision Models</h1>
</header>

<div class="container">
  <section>
    <h2>About the Study</h2>
    <div class="authors">
      <strong>Authors:</strong> Khashayar Ghamati, Maryam Banitalebi Dehkordi, Abolfazl Zaraki<br>
      <strong>Affiliation:</strong> University of Hertfordshire
    </div>
    <p>
      As large language models (LLMs) and vision-language models (VLMs) become increasingly integrated into robotic systems, a fundamental question emerges: <b>to what extent do these models replicate human-like cognitive processes?</b> This study addresses this critical gap by examining the cognitive plausibility of state-of-the-art AI models through the lens of human visual attention patterns in naturalistic human-robot interaction scenarios.
    </p>
    <p>
      Using eye-tracking data as a behavioural proxy for human cognition, we evaluate how well various AI models can predict where humans naturally direct their attention during social interactions. Our comprehensive analysis compares LLaVA (vision-language model) with four large language models (DeepSeek, Qwen, LLaMA, Gemma) across both stateless and memory-augmented conditions to assess their alignment with human visual attention patterns.
    </p>
    <p style="color: #0077b6;">
      <i class="fas fa-lightbulb"></i> <b>Key Insight:</b> While AI models show impressive capabilities, their attention patterns often diverge significantly from human cognitionâ€”revealing the complexity of replicating genuine human-like visual processing.
    </p>
  </section>

  <div class="figure">
    <img src="LLM_comprasion.png" alt="Evaluation pipeline showing vision-language model processing video frames and large language models predicting saliency points compared to human eye-tracking data" loading="eager" width="800" height="600">
    <p><b>Figure 1:</b> Overview of the evaluation pipeline. The vision-language model processes each video frame to generate textual descriptions, which are passed to four large language models. The system operates in two modes: stateless (frame-by-frame) and memory-augmented (STM), where concatenated frame descriptions are provided. Human eye-tracking data serves as ground truth for evaluating model alignment.</p>
  </div>

  <section>
    <h2>Research Motivation: Why Cognitive Plausibility Matters</h2>
    <p>
      The integration of AI models into robotic systems raises fundamental questions about cognitive alignment. While these models demonstrate impressive performance on benchmarks, their ability to replicate the underlying computational processes characteristic of human cognition remains largely unexplored. This is particularly crucial in human-robot interaction, where socially appropriate responses depend on understanding and mimicking human attentional patterns.
    </p>
    <div class="highlight-box">
      <h3>Core Research Questions</h3>
      <ul>
        <li><b>Cognitive Plausibility:</b> Do AI models achieve human-like outcomes through mechanisms that align with human cognitive architecture?</li>
        <li><b>Visual Attention Modeling:</b> Can current AI models predict where humans naturally direct their attention in social scenarios?</li>
        <li><b>Memory Integration:</b> How does short-term memory influence the cognitive plausibility of AI attention predictions?</li>
        <li><b>Model Comparison:</b> Which AI architectures most closely replicate human visual attention patterns?</li>
      </ul>
    </div>
  </section>

  <section>
    <h2>Methodology: Eye-Tracking Meets AI Vision</h2>
    <p>
      Our evaluation framework combines empirical human behaviour data with computational model assessment. We collected eye-tracking data from 11 participants observing a naturalistic human-robot interaction scenario, providing frame-level gaze fixations as ground truth for human visual attention patterns.
    </p>
    <h3>Experimental Setup</h3>
    <ul>
      <li><b>Human Study:</b> 11 participants (mean age 27.3 years) viewed a 7-minute video of dyadic conversation while wearing DIKABLIS eye-tracking equipment</li>
      <li><b>AI Evaluation:</b> The same video was processed by LLaVA for scene description, with outputs fed to four LLMs for saliency prediction</li>
      <li><b>Memory Conditions:</b> Models tested in both stateless (single-frame) and memory-augmented (5-frame context) modes</li>
      <li><b>Alignment Assessment:</b> Cosine similarity between AI predictions and human gaze patterns measured cognitive alignment</li>
    </ul>
  </section>


  <section>
    <h2>Key Findings: Cognitive Alignment Across AI Models</h2>
    <p>
      Our comprehensive evaluation revealed significant differences in how well various AI models align with human visual attention patterns. The results provide important insights for selecting appropriate models for human-robot interaction applications.
    </p>
    
    <h3>Primary Results</h3>
    <ul>
      <li><b>LLaVA (Stateless):</b> Demonstrated the highest alignment with human gaze patterns, suggesting superior visual-linguistic integration for attention modeling</li>
      <li><b>DeepSeek with STM:</b> Only model to benefit from memory augmentation, showing improved performance with temporal context through lexical anchoring mechanisms</li>
      <li><b>Other LLMs:</b> Exhibited degraded performance with memory due to prompt interference and limited contextual integration capabilities</li>
      <li><b>Memory Effects:</b> Short-term memory benefits were model-specific, highlighting the importance of architectural considerations in cognitive plausibility</li>
    </ul>

    <div class="highlight-box">
      <h3>Cognitive Implications</h3>
      <p>
        The divergent performance patterns across models suggest that <b>cognitive plausibility cannot be assumed</b> from general AI capabilities. Models that excel at language tasks may not necessarily replicate human-like attention mechanisms, emphasizing the need for targeted evaluation in specific cognitive domains.
      </p>
    </div>
  </section>

  <section>
    <h2>Technical Innovation: Memory-Augmented Attention Modeling</h2>
    <p>
      Our study introduces a novel framework for assessing cognitive plausibility that goes beyond simple performance matching. By incorporating short-term memory mechanisms and comparing stateless versus memory-augmented conditions, we provide insights into the temporal dynamics of AI attention modeling.
    </p>
    <h3>Short-Term Memory Implementation</h3>
    <ul>
      <li><b>Context Window:</b> 5-frame sliding window reflecting human working memory capacity limitations</li>
      <li><b>Temporal Coherence:</b> Enables models to leverage recurring visual patterns and maintain attentional consistency</li>
      <li><b>Cognitive Constraints:</b> Bounded memory prevents unlimited context accumulation that would violate cognitive plausibility</li>
      <li><b>Interference Effects:</b> Reveals how different architectures handle memory load and contextual integration</li>
    </ul>
  </section>

  <section>
    <h2>Implications for Human-Robot Interaction</h2>
    <p>
      The findings have significant implications for developing socially intelligent robotic systems. Understanding which AI models most closely replicate human attention patterns enables better selection of computational frameworks for HRI applications, potentially improving robot social awareness and interaction quality.
    </p>
    <ul>
      <li><b>Model Selection:</b> Provides empirical guidance for choosing appropriate AI models for robot perception systems</li>
      <li><b>Social Robotics:</b> Enables development of robots with more human-like attention allocation in social contexts</li>
      <li><b>Cognitive Alignment:</b> Establishes benchmarks for evaluating AI system alignment with human cognitive processes</li>
      <li><b>Adaptive Systems:</b> Informs design of AI agents that can adapt their attention patterns based on human-like principles</li>
    </ul>
  </section>

  <section>
    <h2>Future Directions and Limitations</h2>
    <p>
      While this study establishes a foundational framework for cognitive plausibility assessment, several limitations and future directions merit consideration. The controlled experimental setting, while enabling rigorous comparison, limits immediate generalizability to diverse HRI contexts.
    </p>
    <h3>Study Limitations</h3>
    <ul>
      <li><b>Scenario Scope:</b> Single dyadic interaction scenario limits generalizability to complex multi-agent environments</li>
      <li><b>Participant Demographics:</b> Homogeneous sample (engineering students) may not represent diverse population attention patterns</li>
      <li><b>Temporal Constraints:</b> Memory implementations constrained by computational limitations across different models</li>
    </ul>
    <h3>Future Research</h3>
    <ul>
      <li><b>Diverse Scenarios:</b> Evaluation across varied interaction contexts, task-oriented behaviours, and cultural settings</li>
      <li><b>Extended Memory:</b> Investigation of longer-term memory effects and learning mechanisms</li>
      <li><b>Real-World Validation:</b> Field studies in naturalistic HRI environments</li>
    </ul>
  </section>

  <section>
    <h2>Open Science & Collaboration</h2>
    <p>
      We support open, reproducible research and welcome collaborations to advance understanding of cognitive plausibility in AI systems. This work provides a foundation for developing more human-aligned AI agents for robotics and interactive systems.
      <span style="font-size:1.2em;">
      <a href="https://scholar.google.com/citations?user=zRG8t_oAAAAJ&hl=en" target="_blank" rel="noopener" title="Google Scholar" style="margin-right:10px;">
        <i class="fas fa-graduation-cap"></i> Google Scholar
      </a>
      <a href="https://orcid.org/0009-0002-6416-3127" target="_blank" rel="noopener" title="ORCID">
        <i class="fab fa-orcid"></i> ORCID
      </a>
    </span>
    </p>
  </section>

  <section>
    <h2>Full Paper & Resources</h2>
    <a href="#" class="download-btn" onclick="showPaperDialog(event)">
      <i class="fas fa-download"></i> Download Full Paper
    </a>
    <a href="#" class="download-btn" onclick="showCodeDialog(event)">
      <i class="fas fa-code"></i> Code & Data
    </a>
    <p style="margin-top:1rem;font-size:1rem;color:#0077b6;">
      <i class="fas fa-info-circle"></i> This research contributes to the growing field of cognitive AI and human-robot interaction.
    </p>
  </section>
</div>

<script>
  function showCodeDialog(event) {
    event.preventDefault();
    alert("To access the source code, please email us at: k.ghamati@herts.ac.uk");
  }

  function showPaperDialog(event) {
    event.preventDefault();
    alert("Full paper will be available upon publication. For early access, please contact: k.ghamati@herts.ac.uk");
  }
</script>

<footer>
  <p>&copy; 2025 Khashayar Ghamati. All rights reserved.</p>
</footer>

</body>
</html>