<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- SEO Meta Tags -->
  <meta name="description" content="Personalised Large Language Model (PLLM) for Human-Robot and Human-Computer Interaction: Real-time emotion-aware adaptation with EEG signals. Published in MDPI Sensors 2025 by Khashayar Ghamati et al.">
  <meta name="keywords" content="personalised large language model, PLLM, EEG, human-robot interaction, HRI, human-computer interaction, HCI, adaptive AI, emotion recognition, MDPI Sensors, Khashayar Ghamati, Streamlit, real-time AI, neuroadaptive AI, OpenAI, agentic AI, personalisation, machine learning, affective computing, continual learning, dialogue system, emotion-adaptive chatbot, LLM fine-tuning, multi-modal AI">
  <meta name="author" content="Khashayar Ghamati, Maryam Banitalebi Dehkordi, Abolfazl Zaraki">
  <meta name="robots" content="index, follow">

  <!-- Open Graph (for social and rich previews) -->
  <meta property="og:title" content="Personalised LLM for HRI/HCI: EEG-Driven Emotion Recognition | Sensors 2025">
  <meta property="og:description" content="Published in MDPI Sensors 2025: Personalised Large Language Model (PLLM) enables real-time, emotion-adaptive dialogue in HRI/HCI using EEG data. Authors: Khashayar Ghamati, Maryam Banitalebi Dehkordi, Abolfazl Zaraki.">
  <meta property="og:image" content="https://ghamati.com/pllm/f1.jpg">
  <meta property="og:url" content="https://ghamati.com/pllm/">
  <meta property="article:published_time" content="2025-03-24">
  <meta property="article:author" content="Khashayar Ghamati">
  <meta property="article:tag" content="personalised LLM, EEG, HRI, HCI, emotion-adaptive chatbot, adaptive AI, Sensors 2025">

  <!-- Canonical Link -->
  <link rel="canonical" href="https://ghamati.com/pllm/">

  <title>Personalised LLM for Emotion-Aware HRI and HCI | Sensors 2025 | Khashayar Ghamati</title>

  <!-- Google Fonts -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap">

  <!-- External CSS for Icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "ScholarlyArticle",
      "headline": "Towards AI-Powered Applications: The Development of a Personalised LLM for HRI and HCI",
      "author": [
        {
          "@type": "Person",
          "name": "Khashayar Ghamati",
          "affiliation": "University of Hertfordshire"
        },
        {
          "@type": "Person",
          "name": "Maryam Banitalebi Dehkordi"
        },
        {
          "@type": "Person",
          "name": "Abolfazl Zaraki"
        }
      ],
      "datePublished": "2025-03-24",
      "publisher": {
        "@type": "Organization",
        "name": "MDPI Sensors"
      },
      "about": [
        "personalised LLM",
        "EEG",
        "human-robot interaction",
        "emotion recognition",
        "adaptive AI",
        "machine learning",
        "affective computing",
        "continual learning"
      ],
      "description": "This work presents a Personalised Large Language Model (PLLM) for real-time emotion-adaptive dialogue, integrating EEG data to personalise LLM behaviour in HRI/HCI. Published in MDPI Sensors 2025."
    }
  </script>

  <style>
    body { font-family: 'Roboto', sans-serif; margin: 0; padding: 0; background-color: #f5f7fa; color: #333; }
    .toolbar { background-color: #0077b6; padding: 1rem; position: sticky; top: 0; z-index: 1000; display: flex; justify-content: flex-start; box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);}
    .toolbar a { color: white; text-decoration: none; font-size: 1.125rem; font-weight: 400; margin-right: 20px; transition: color 0.3s;}
    .toolbar a:hover { color: #94d2bd;}
    header { background-color: #0077b6; padding: 2rem 0; text-align: center; color: white; box-shadow: 0 2px 10px rgba(0,0,0,0.1);}
    header h1 { font-size: 2.5rem; margin: 0; font-weight: 700;}
    .container { max-width: 1200px; margin: 2rem auto; padding: 0 20px;}
    section h2 { font-size: 2rem; color: #0077b6; text-align: center; margin-bottom: 1.5rem; font-weight: 700;}
    section p { font-size: 1.125rem; line-height: 1.6; text-align: justify; margin-bottom: 2rem;}
    h3 { color: #333; font-size: 1.5rem; margin-bottom: 1rem;}
    ul { font-size: 1.125rem; line-height: 1.6;}
    .figure { text-align: center; margin: 20px 0;}
    .figure img { max-width: 50%; height: auto; border-radius: 8px; box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);}
    a.download-btn { display: inline-block; background-color: #0077b6; color: white; padding: 0.75rem 1.5rem; text-decoration: none; font-size: 1rem; border-radius: 5px; transition: background-color 0.3s; text-align: center; margin-top: 2rem; box-shadow: 0 4px 10px rgba(0,0,0,0.1);}
    a.download-btn:hover { background-color: #0096c7;}
    footer { text-align: center; padding: 2rem 0; background-color: #023e8a; color: white; margin-top: 3rem; box-shadow: 0 -2px 10px rgba(0,0,0,0.1);}
  </style>
</head>
<body>

<div class="toolbar">
  <a href="/"><i class="fas fa-home"></i> Home</a>
</div>

<header>
  <h1>Personalised LLM for Emotion-Aware Human-Robot and Human-Computer Interaction</h1>
</header>

<div class="container">
  <section id="about">
    <h2>About This Work</h2>
    <p>
      <strong>Towards AI-Powered Applications: The Development of a Personalised LLM for HRI and HCI</strong>
      (published in <b>MDPI Sensors 2025</b>) introduces a <b>Personalised Large Language Model (PLLM)</b> capable of real-time adaptation to user emotions in both <b>Human-Robot Interaction (HRI)</b> and <b>Human-Computer Interaction (HCI)</b>. The PLLM framework integrates EEG-based emotion recognition with dialogue generation, enabling context- and emotion-aware responses tailored to individual users.
    </p>
    <p>
      The study demonstrates that personalising LLMs with domain-specific signals—here, EEG-derived emotion states—can substantially improve the relevance and impact of AI-generated responses in dynamic, interactive settings.
    </p>
    <p>
      <strong>Authors:</strong> Khashayar Ghamati, Maryam Banitalebi Dehkordi, Abolfazl Zaraki <br>
      <strong>Published:</strong> <a href="https://doi.org/10.3390/s25072024" target="_blank" rel="noopener">Sensors 2025, 25(7), 2024</a> | <b>Open Access</b>
    </p>
    <p>
      <a href="https://scholar.google.com/citations?user=zRG8t_oAAAAJ&hl=en" target="_blank" title="Google Scholar"><i class="fas fa-graduation-cap"></i> Google Scholar</a> |
      <a href="https://orcid.org/0009-0002-6416-3127" target="_blank" title="ORCID"><i class="fab fa-orcid"></i> ORCID</a> |
      <a href="https://www.mdpi.com/journal/sensors" target="_blank" title="MDPI Sensors"><i class="fas fa-globe"></i> MDPI Sensors</a>
    </p>
  </section>

  <section id="framework">
    <h2>Framework Overview & Methodology</h2>
    <div class="figure">
      <img src="f1.jpg" alt="PLLM framework: architecture for EEG-driven emotion-adaptive AI dialogue agent (Sensors 2025)">
      <p><strong>Figure 1:</strong> Overview of the PLLM framework: EEG signals are analysed in real-time, classified into emotional states, and used to condition LLM-based dialogue. The user receives personalised, emotion-aware responses via a Streamlit web interface.</p>
    </div>
    <p>
      The PLLM system consists of three main components:
    <ul>
      <li><b>EEG-based Emotion Recognition:</b> The agent processes raw EEG signals (using the NeuroSense dataset) to classify the user's current emotional state in real time, utilising established deep learning pipelines.</li>
      <li><b>LLM Personalisation:</b> The emotional state output conditions the prompts for the large language model (LLM), allowing dialogue and actions to reflect both situational context and user affect.</li>
      <li><b>Interactive Application:</b> The framework is deployed via <b>Streamlit</b>, making it accessible for real-world HRI/HCI experiments and end-users.</li>
    </ul>
    </p>
    <div class="figure">
      <img src="f2.jpg" alt="Integration of EEG-based emotion recognition with PLLM dialogue agent (Sensors 2025)">
      <p><strong>Figure 2:</strong> Real-time integration of emotion prediction and dialogue generation—showing the feedback loop from EEG, through emotion recognition, to context-aware, adaptive AI responses.</p>
    </div>
  </section>

  <section id="results">
    <h2>Experimental Results & Key Findings</h2>
    <p>
      Experiments were performed using the NeuroSense dataset for emotion annotation and validation. The results demonstrate that:
    <ul>
      <li>The PLLM agent achieves <b>high accuracy</b> in real-time EEG-based emotion detection.</li>
      <li>Emotion-driven prompts substantially improved the personalisation and appropriateness of AI-generated dialogue compared to baseline LLMs.</li>
      <li>Subjective user studies indicated increased engagement and perceived intelligence during interaction.</li>
    </ul>
    For full results and figures (including confusion matrices, scenario demos, and user feedback), see the open-access <a href="https://doi.org/10.3390/s25072024" target="_blank" rel="noopener">Sensors 2025 article</a>.
    </p>
  </section>

  <section id="novelty">
    <h2>Novelty & Contribution</h2>
    <ul>
      <li><b>First</b> demonstration of real-time EEG-driven personalisation in LLM-based HRI/HCI dialogue.</li>
      <li><b>Open-source</b> Streamlit application for replicability and rapid prototyping.</li>
      <li>Highlights the value of <b>affective computing</b> in AI-powered interaction, and offers a path for integrating neural signals with language models for emotion-adaptive robots and systems.</li>
    </ul>
    <p>
      This work lays the groundwork for future neuroadaptive AI agents and can be extended to other multimodal contexts or continuous adaptation scenarios.
    </p>
  </section>

  <section id="availability">
    <h2>Code & Data Availability</h2>
    <p>
      The NeuroSense dataset and full details are provided in the published article. For source code or collaborations, please contact us.
    </p>
    <a href="https://www.mdpi.com/1424-8220/25/7/2024" target="_blank" class="download-btn">Download Full Paper</a>
    <a href="#" class="download-btn" onclick="showCodeDialog(event)">Code</a>
    <p style="margin-top:1rem;font-size:1rem;color:#0077b6;">
      <i class="fas fa-info-circle"></i> The article is open access under the <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" style="color:#0077b6;text-decoration:underline;">CC BY 4.0 license</a>.
    </p>
  </section>
</div>

<script>
  function showCodeDialog(event) {
    event.preventDefault();
    alert("To access the source code, please email us at: k.ghamati@herts.ac.uk");
  }
</script>

<footer>
  <p>&copy; 2025 Khashayar Ghamati. All rights reserved.</p>
</footer>
</body>
</html>
