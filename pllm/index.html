<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <!-- DNS Prefetch and Preconnect for Performance -->
  <link rel="dns-prefetch" href="//fonts.googleapis.com">
  <link rel="dns-prefetch" href="//cdnjs.cloudflare.com">
  <link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  
  <!-- Enhanced SEO Meta Tags -->
  <meta name="description" content="Personalised Large Language Model (PLLM) for emotion-aware human-robot and human-computer interaction using real-time EEG signals. Published in MDPI Sensors 2025 by Khashayar Ghamati et al.">
  <meta name="keywords" content="personalised large language model, PLLM, EEG, emotion recognition, human-robot interaction, HRI, human-computer interaction, HCI, adaptive AI, MDPI Sensors 2025, Khashayar Ghamati, Streamlit, real-time AI, neuroadaptive AI, OpenAI, agentic AI, personalisation, machine learning, affective computing, continual learning, dialogue system, emotion-adaptive chatbot, LLM fine-tuning, multi-modal AI, NeuroSense dataset">
  <meta name="author" content="Khashayar Ghamati, Maryam Banitalebi Dehkordi, Abolfazl Zaraki">
  <meta name="robots" content="index, follow">

  <!-- Open Graph for social and richer search results -->
  <meta property="og:title" content="Personalised LLM for Emotion-Aware HRI/HCI | MDPI Sensors 2025">
  <meta property="og:description" content="Breakthrough emotion-adaptive AI using EEG signals for real-time personalisation. PLLM enables context-aware dialogue in human-robot interaction published in MDPI Sensors 2025.">
  <meta property="og:image" content="https://ghamati.com/pllm/f1.jpg">
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://ghamati.com/pllm/">
  <meta property="og:site_name" content="Khashayar Ghamati - AI Research">
  <meta property="article:author" content="Khashayar Ghamati">
  <meta property="article:published_time" content="2025-03-24">
  <meta property="article:section" content="Artificial Intelligence">
  <meta property="article:tag" content="personalised AI">
  <meta property="article:tag" content="EEG">
  <meta property="article:tag" content="emotion recognition">
  <meta property="article:tag" content="human-robot interaction">

  <!-- Twitter Card -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Personalised LLM for Emotion-Aware HRI and HCI | MDPI Sensors 2025">
  <meta name="twitter:description" content="Revolutionary EEG-driven personalisation in LLM-based dialogue for human-robot interaction, published in MDPI Sensors 2025.">
  <meta name="twitter:image" content="https://ghamati.com/pllm/f1.jpg">
  <meta name="twitter:creator" content="@khashayarghamati">

  <link rel="canonical" href="https://ghamati.com/pllm/">

  <title>Personalised LLM for Emotion-Aware Human-Robot Interaction | MDPI Sensors 2025</title>

  <!-- Google Fonts -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Playfair+Display:wght@400;600;700&display=swap">

  <!-- External CSS for Icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer">

  <!-- Enhanced Structured Data -->
  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "ScholarlyArticle",
      "headline": "Towards AI-Powered Applications: The Development of a Personalised LLM for HRI and HCI",
      "author": [
        {
          "@type": "Person",
          "name": "Khashayar Ghamati",
          "affiliation": {
            "@type": "Organization",
            "name": "University of Hertfordshire"
          },
          "url": "https://ghamati.com",
          "sameAs": [
            "https://scholar.google.com/citations?user=zRG8t_oAAAAJ&hl=en",
            "https://orcid.org/0009-0002-6416-3127"
          ]
        },
        {
          "@type": "Person",
          "name": "Maryam Banitalebi Dehkordi",
          "affiliation": {
            "@type": "Organization",
            "name": "University of Hertfordshire"
          }
        },
        {
          "@type": "Person",
          "name": "Abolfazl Zaraki",
          "affiliation": {
            "@type": "Organization",
            "name": "University of Hertfordshire"
          }
        }
      ],
      "datePublished": "2025-03-24",
      "url": "https://ghamati.com/pllm/",
      "image": "https://ghamati.com/pllm/f1.jpg",
      "inLanguage": "en",
      "publisher": {
        "@type": "Organization",
        "name": "MDPI Sensors",
        "url": "https://www.mdpi.com/journal/sensors"
      },
      "about": [
        "personalised LLM",
        "EEG",
        "human-robot interaction",
        "emotion recognition",
        "adaptive AI",
        "machine learning",
        "affective computing",
        "continual learning",
        "neuroadaptive AI",
        "dialogue systems"
      ],
      "description": "This work presents a Personalised Large Language Model (PLLM) for real-time emotion-adaptive dialogue, integrating EEG data to personalise LLM behaviour in HRI/HCI. Published in MDPI Sensors 2025.",
      "keywords": "personalised LLM, EEG, emotion recognition, human-robot interaction, adaptive AI, neuroadaptive systems",
      "isAccessibleForFree": true,
      "license": "https://creativecommons.org/licenses/by/4.0/",
      "creativeWorkStatus": "Published",
      "citation": "Ghamati, K.; Dehkordi, M.B.; Zaraki, A. Towards AI-Powered Applications: The Development of a Personalised LLM for HRI and HCI. Sensors 2025, 25, 2024. https://doi.org/10.3390/s25072024"
    }
  </script>

  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-HV1K905FF2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-HV1K905FF2');
  </script>

  <style>
    :root {
      /* Warm Color Palette */
      --primary-warm: #D2691E;
      --primary-warm-light: #E6A85C;
      --primary-warm-dark: #B8860B;
      --secondary-warm: #CD853F;
      --accent-warm: #F4A460;
      --accent-gold: #DAA520;
      
      /* Neutral Warm Tones */
      --bg-warm: #FDF6E3;
      --bg-card: #FFFFFF;
      --text-primary: #2F1B14;
      --text-secondary: #5D4037;
      --text-muted: #8D6E63;
      
      /* Gradients */
      --gradient-warm: linear-gradient(135deg, var(--primary-warm) 0%, var(--secondary-warm) 100%);
      --gradient-gold: linear-gradient(135deg, var(--accent-gold) 0%, var(--accent-warm) 100%);
      
      /* Typography */
      --font-heading: 'Playfair Display', serif;
      --font-body: 'Inter', sans-serif;
      
      /* Shadows */
      --shadow-soft: 0 4px 20px rgba(210, 105, 30, 0.1);
      --shadow-medium: 0 8px 30px rgba(210, 105, 30, 0.15);
      --shadow-strong: 0 12px 40px rgba(210, 105, 30, 0.2);
    }

    *, *::before, *::after {
      box-sizing: border-box;
    }
    
    body {
      font-family: var(--font-body);
      margin: 0;
      padding: 0;
      background: var(--bg-warm);
      color: var(--text-primary);
      line-height: 1.7;
      font-size: 16px;
    }

    /* Toolbar Styling */
    .toolbar {
      background: var(--gradient-warm);
      padding: 1rem 0;
      position: sticky;
      top: 0;
      z-index: 1000;
      box-shadow: var(--shadow-medium);
      backdrop-filter: blur(10px);
    }

    .toolbar .container {
      display: flex;
      justify-content: flex-start;
      align-items: center;
      margin: 0;
      padding: 0 2rem;
    }

    .toolbar a {
      color: white;
      text-decoration: none;
      font-size: 1rem;
      font-weight: 500;
      padding: 0.5rem 1rem;
      border-radius: 25px;
      transition: all 0.3s ease;
      display: flex;
      align-items: center;
      gap: 0.5rem;
    }

    .toolbar a:hover {
      background: rgba(255, 255, 255, 0.2);
      transform: translateY(-2px);
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
    }

    /* Header Styling */
    header {
      background: var(--gradient-warm);
      padding: 4rem 0;
      text-align: center;
      color: white;
      position: relative;
      overflow: hidden;
    }

    header::before {
      content: '';
      position: absolute;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1000 300"><path fill="%23ffffff" fill-opacity="0.1" d="M0,96L48,112C96,128,192,160,288,170.7C384,181,480,171,576,154.7C672,139,768,117,864,117.3C960,117,1056,139,1152,149.3C1248,160,1344,160,1392,160L1440,160L1440,320L1392,320C1344,320,1248,320,1152,320C1056,320,960,320,864,320C768,320,672,320,576,320C480,320,384,320,288,320C192,320,96,320,48,320L0,320Z"/></svg>') repeat-x bottom;
      background-size: 1440px 160px;
    }

    header h1 {
      font-family: var(--font-heading);
      font-size: clamp(2rem, 5vw, 3.5rem);
      margin: 0;
      font-weight: 700;
      text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.1);
      line-height: 1.2;
      max-width: 1200px;
      margin: 0 auto;
      padding: 0 2rem;
    }

    /* Container */
    .container {
      max-width: 1200px;
      margin: 0 auto;
      padding: 0 2rem;
    }

    /* Main Content */
    main {
      padding: 4rem 0;
    }

    /* Section Styling */
    section {
      background: var(--bg-card);
      margin: 3rem 0;
      padding: 3rem;
      border-radius: 20px;
      box-shadow: var(--shadow-soft);
      position: relative;
      overflow: hidden;
    }

    section h2 {
      font-family: var(--font-heading);
      font-size: 2.5rem;
      color: var(--primary-warm);
      text-align: center;
      margin: 0 0 2rem 0;
      font-weight: 600;
      position: relative;
    }

    section h2::after {
      content: '';
      display: block;
      width: 80px;
      height: 4px;
      background: var(--gradient-gold);
      margin: 1rem auto;
      border-radius: 2px;
    }

    section h3 {
      font-family: var(--font-heading);
      color: var(--text-primary);
      font-size: 1.8rem;
      margin: 2rem 0 1rem 0;
      font-weight: 600;
    }

    section p {
      font-size: 1.1rem;
      line-height: 1.8;
      color: var(--text-secondary);
      margin-bottom: 1.5rem;
      text-align: left;
    }

    /* Enhanced Lists */
    ul {
      list-style: none;
      padding: 0;
    }

    ul li {
      position: relative;
      padding: 0.8rem 0 0.8rem 2rem;
      margin: 0.5rem 0;
      font-size: 1.1rem;
      line-height: 1.7;
      color: var(--text-secondary);
    }

    ul li::before {
      content: '→';
      position: absolute;
      left: 0;
      color: var(--primary-warm);
      font-weight: bold;
      font-size: 1.2rem;
    }

    /* Figure Styling */
    .figure {
      text-align: center;
      margin: 3rem 0;
      background: var(--bg-card);
      padding: 2rem;
      border-radius: 15px;
      box-shadow: var(--shadow-soft);
    }

    .figure img {
      max-width: 100%;
      height: auto;
      border-radius: 12px;
      box-shadow: var(--shadow-medium);
      transition: transform 0.3s ease;
    }

    .figure img:hover {
      transform: scale(1.02);
    }

    .figure p {
      margin-top: 1.5rem;
      font-style: italic;
      color: var(--text-muted);
      font-size: 0.95rem;
      line-height: 1.6;
    }

    .figure-large img {
      max-width: 80%;
    }

    /* Highlight Box */
    .highlight {
      background: linear-gradient(135deg, var(--accent-warm) 0%, var(--accent-gold) 100%);
      color: white;
      padding: 2rem;
      border-radius: 15px;
      margin: 2rem 0;
      box-shadow: var(--shadow-medium);
    }

    .highlight .fas {
      font-size: 1.2rem;
      margin-right: 0.5rem;
    }

    /* Authors Section */
    .authors {
      background: linear-gradient(135deg, rgba(210, 105, 30, 0.1) 0%, rgba(218, 165, 32, 0.1) 100%);
      padding: 1.5rem;
      border-radius: 12px;
      margin: 2rem 0;
      text-align: center;
      border-left: 4px solid var(--primary-warm);
    }

    .authors p {
      margin: 0.5rem 0;
      font-weight: 500;
    }

    /* Publication Info */
    .publication-info {
      background: var(--bg-card);
      border: 2px solid var(--accent-warm);
      border-radius: 15px;
      padding: 2rem;
      margin: 2rem 0;
      text-align: center;
    }

    .publication-info .journal {
      color: var(--primary-warm);
      font-weight: 600;
      font-size: 1.2rem;
    }

    .publication-info .license {
      background: #e8f5e8;
      color: #2e7d32;
      padding: 0.5rem 1rem;
      border-radius: 20px;
      font-size: 0.9rem;
      display: inline-block;
      margin-top: 1rem;
    }

    /* Download Buttons */
    .download-section {
      text-align: center;
      padding: 3rem 0;
    }

    .download-btn {
      display: inline-block;
      background: var(--gradient-warm);
      color: white;
      padding: 1rem 2rem;
      margin: 0.5rem;
      text-decoration: none;
      font-size: 1.1rem;
      font-weight: 500;
      border-radius: 50px;
      transition: all 0.3s ease;
      box-shadow: var(--shadow-soft);
      position: relative;
      overflow: hidden;
    }

    .download-btn.primary {
      background: var(--gradient-gold);
    }

    .download-btn::before {
      content: '';
      position: absolute;
      top: 0;
      left: -100%;
      width: 100%;
      height: 100%;
      background: linear-gradient(90deg, transparent, rgba(255, 255, 255, 0.2), transparent);
      transition: left 0.5s;
    }

    .download-btn:hover {
      transform: translateY(-3px);
      box-shadow: var(--shadow-strong);
    }

    .download-btn:hover::before {
      left: 100%;
    }

    /* Footer */
    footer {
      background: var(--gradient-warm);
      color: white;
      text-align: center;
      padding: 3rem 0;
      margin-top: 4rem;
    }

    /* Social Links */
    .social-links {
      display: flex;
      justify-content: center;
      gap: 1rem;
      margin: 2rem 0;
    }

    .social-links a {
      color: var(--primary-warm);
      font-size: 1.5rem;
      transition: all 0.3s ease;
    }

    .social-links a:hover {
      color: var(--accent-gold);
      transform: translateY(-2px);
    }

    /* Responsive Design */
    @media (max-width: 768px) {
      .container {
        padding: 0 1rem;
      }
      
      section {
        padding: 2rem 1.5rem;
        margin: 2rem 0;
      }
      
      header {
        padding: 2rem 0;
      }
      
      section h2 {
        font-size: 2rem;
      }
      
      .figure {
        padding: 1rem;
      }
      
      .download-btn {
        display: block;
        margin: 1rem auto;
        max-width: 280px;
      }
    }

    /* Animations */
    @keyframes fadeInUp {
      from {
        opacity: 0;
        transform: translateY(30px);
      }
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }

    section {
      animation: fadeInUp 0.6s ease-out;
    }
  </style>
</head>
<body>

<!-- Toolbar with Home Button -->
<div class="toolbar">
  <div class="container">
    <a href="/"><i class="fas fa-home"></i> Home</a>
  </div>
</div>

<header>
  <h1>Personalised LLM for Emotion-Aware Human-Robot and Human-Computer Interaction</h1>
</header>

<main>
<div class="container">
  <section>
    <h2>About This Work</h2>
    <div class="authors">
      <p><strong>Authors:</strong> Khashayar Ghamati, Maryam Banitalebi Dehkordi, Abolfazl Zaraki</p>
      <p><strong>Affiliation:</strong> University of Hertfordshire, UK</p>
    </div>
    
    <div class="publication-info">
      <p class="journal">Published in MDPI Sensors 2025</p>
      <p><strong>DOI:</strong> <a href="https://doi.org/10.3390/s25072024" target="_blank" rel="noopener">10.3390/s25072024</a></p>
      <div class="license">
        <i class="fas fa-creative-commons"></i> Open Access - CC BY 4.0
      </div>
    </div>

    <p>
      <strong>Towards AI-Powered Applications: The Development of a Personalised LLM for HRI and HCI</strong> introduces a groundbreaking <b>Personalised Large Language Model (PLLM)</b> framework that achieves real-time adaptation to user emotions in both <b>Human-Robot Interaction (HRI)</b> and <b>Human-Computer Interaction (HCI)</b>. The PLLM integrates EEG-based emotion recognition with advanced dialogue generation, enabling context-aware and emotion-sensitive responses tailored to individual users.
    </p>
    
    <div class="highlight">
      <i class="fas fa-brain"></i> <b>Innovation:</b> This is the first demonstration of real-time EEG-driven personalisation in LLM-based dialogue systems, opening new frontiers in neuroadaptive AI.
    </div>
  </section>

  <div class="figure">
    <img src="f1.jpg" alt="PLLM framework architecture showing EEG-driven emotion-adaptive AI dialogue system for human-robot interaction" loading="eager" width="800" height="600">
    <p><b>Figure 1:</b> Overview of the PLLM framework architecture. EEG signals are processed in real-time for emotion classification, which then conditions the large language model to generate personalised, emotion-aware responses through a user-friendly Streamlit interface.</p>
  </div>

  <section>
    <h2>Framework Architecture & Methodology</h2>
    <p>
      The PLLM system represents a paradigm shift in adaptive AI, combining neuroscience with cutting-edge language models to create truly personalised interactions. Our framework consists of three integrated components working in seamless harmony:
    </p>
    
    <h3>Core Components</h3>
    <ul>
      <li><b>EEG-based Emotion Recognition:</b> Real-time processing of neural signals using the NeuroSense dataset, employing advanced deep learning pipelines to classify emotional states with high accuracy.</li>
      <li><b>Dynamic LLM Conditioning:</b> Emotional state outputs directly influence prompt engineering, allowing the language model to adapt its tone, content, and interaction style based on the user's current affective state.</li>
      <li><b>Interactive Deployment Platform:</b> A sophisticated Streamlit application provides real-world accessibility, enabling researchers and end-users to experience emotion-adaptive AI in practice.</li>
    </ul>
    
    <div class="highlight">
      <i class="fas fa-cogs"></i> <b>Technical Achievement:</b> The system processes EEG signals and generates contextually appropriate responses within milliseconds, demonstrating the feasibility of real-time neuroadaptive AI.
    </div>
  </section>

  <div class="figure">
    <img src="f2.jpg" alt="Real-time integration pipeline showing EEG emotion recognition feeding into personalised LLM dialogue generation" loading="lazy" width="800" height="600">
    <p><b>Figure 2:</b> Real-time integration pipeline demonstrating the closed-loop system from EEG signal acquisition through emotion classification to adaptive dialogue generation, showcasing the seamless flow of neuroadaptive personalisation.</p>
  </div>

  <section>
    <h2>Experimental Results & Validation</h2>
    <p>
      Our comprehensive evaluation using the NeuroSense dataset demonstrates the effectiveness of emotion-driven personalisation in AI dialogue systems. The results establish new benchmarks for neuroadaptive human-computer interaction:
    </p>
    
    <h3>Key Findings</h3>
    <ul>
      <li><b>High-Accuracy Emotion Detection:</b> The EEG-based emotion recognition module achieves robust performance in real-time classification across multiple emotional states.</li>
      <li><b>Enhanced Dialogue Quality:</b> Emotion-conditioned prompts significantly improve the relevance, appropriateness, and user satisfaction compared to baseline language models.</li>
      <li><b>User Engagement Metrics:</b> Subjective evaluations reveal substantially increased engagement and perceived intelligence during emotion-adaptive interactions.</li>
      <li><b>Real-World Feasibility:</b> The Streamlit deployment demonstrates practical applicability for research and commercial applications.</li>
    </ul>
    
    <div class="highlight">
      <i class="fas fa-chart-line"></i> <b>Impact:</b> This work establishes the foundational framework for next-generation neuroadaptive AI systems, with applications spanning healthcare, education, entertainment, and assistive technologies.
    </div>
  </section>

  <section>
    <h2>Scientific Contribution & Novelty</h2>
    <p>
      Our research addresses a critical gap in adaptive AI systems by introducing the first practical framework for EEG-driven LLM personalisation. This work contributes to multiple domains:
    </p>
    
    <h3>Research Contributions</h3>
    <ul>
      <li><b>Neuroadaptive AI Architecture:</b> Novel integration of brain-computer interfaces with large language models for real-time personalisation.</li>
      <li><b>Emotion-Aware Dialogue Systems:</b> Demonstration of how affective states can enhance AI communication quality and user experience.</li>
      <li><b>Open-Source Implementation:</b> Reproducible research framework enabling widespread adoption and extension by the research community.</li>
      <li><b>Cross-Domain Applications:</b> Validated approach applicable to both HRI and HCI contexts, broadening the impact across multiple fields.</li>
    </ul>
    
    <div class="highlight">
      <i class="fas fa-lightbulb"></i> <b>Future Implications:</b> This work paves the way for emotionally intelligent AI assistants, therapeutic robots, and adaptive learning systems that respond to users' mental states in real-time.
    </div>
  </section>

  <section>
    <h2>Open Science & Collaboration</h2>
    <p>
      We are committed to advancing open, reproducible science in neuroadaptive AI. This research provides a foundation for collaborative development of more sophisticated emotion-aware systems and welcomes partnerships to extend these capabilities.
    </p>
    
    <div class="social-links">
      <a href="https://scholar.google.com/citations?user=zRG8t_oAAAAJ&hl=en" target="_blank" rel="noopener" title="Google Scholar">
        <i class="fas fa-graduation-cap"></i>
      </a>
      <a href="https://orcid.org/0009-0002-6416-3127" target="_blank" rel="noopener" title="ORCID">
        <i class="fab fa-orcid"></i>
      </a>
      <a href="https://www.mdpi.com/journal/sensors" target="_blank" rel="noopener" title="MDPI Sensors">
        <i class="fas fa-globe"></i>
      </a>
    </div>
  </section>

  <section>
    <h2>Full Paper & Code Access</h2>
    <div class="download-section">
      <a href="https://www.mdpi.com/1424-8220/25/7/2024" target="_blank" class="download-btn primary">
        <i class="fas fa-download"></i> Download Full Paper (Open Access)
      </a>
      <a href="#" class="download-btn" onclick="showCodeDialog(event)">
        <i class="fas fa-code"></i> Access Source Code
      </a>
    </div>
    <p style="margin-top:2rem;font-size:1rem;color:var(--text-muted);text-align:center;">
      <i class="fas fa-info-circle"></i> The article is published under the <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" style="color:var(--primary-warm);">CC BY 4.0 license</a>, ensuring open access for the research community.
    </p>
  </section>
</div>
</main>

<script>
  function showCodeDialog(event) {
    event.preventDefault();
    alert("To access the source code and datasets, please email us at: k.ghamati@herts.ac.uk\n\nWe're happy to share our implementation and support collaborative research!");
  }
</script>

<footer>
  <div class="container">
    <p>&copy; 2025 Khashayar Ghamati. All rights reserved.</p>
    <p style="font-size: 0.9rem; opacity: 0.8; margin-top: 1rem;">Advancing neuroadaptive AI for human-centered technology</p>
  </div>
</footer>

</body>
</html>